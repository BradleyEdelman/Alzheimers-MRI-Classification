{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885d8509-03bf-42c0-8693-3025249593d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Alzheimer's Disease classification from anatomical MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cf98b28-d111-4bd3-ae7a-4cbb1db88ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook explores the use of a low-dimensional feature space to classify Alzheimer's disease from anatomical MRI images.\n",
    " \n",
    "Briefly, the pipeline involves the following steps and technical features:\n",
    "\n",
    "- Data formating and quality check\n",
    "- Feature extraction and visualization\n",
    "- Synthetic class balancing\n",
    "- Random forest classification as a function of features (principal components) used\n",
    "- Permutation testing for statistical significance (parallel processing)\n",
    "- Feature interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e64237c-6420-4fee-9bb0-7591bd6ed724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import analysis and plotting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62588c7-7321-4b0c-9628-a47a287ae8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning and statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from scipy.stats import false_discovery_control\n",
    "\n",
    "# Parallel computing\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import cv2\n",
    "import magic\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c59f92-08e0-4513-a2a4-4d555fff5d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply random forest classifier to first two PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c7c4cc-e618-4b93-bf05-fd0221d86378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train random forest classifier and apply to test data\n",
    "## for now again only with first two PCs\n",
    "\n",
    "rf_1 = RandomForestClassifier()\n",
    "\n",
    "# Train classifier\n",
    "load_train = np.matmul(train_data, loadings[:,:2])\n",
    "rf_1.fit(load_train, train_lab_idx)\n",
    "\n",
    "# Properly format test data and extract same features (PC loadings)\n",
    "test_data = []\n",
    "for i in range(len(test)):\n",
    "    test_data.append(np.reshape(test.iloc[i].img_arr, (1, -1)))    \n",
    "test_data = np.vstack(test_data[:])\n",
    "\n",
    "load_test = np.matmul(test_data, loadings[:,:2])\n",
    "\n",
    "# Predict unseen data\n",
    "predictions_1 = rf_1.predict(load_test)\n",
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "\n",
    "# compare predictions with test labels and compute accuracy\n",
    "result = predictions_1 - test_lab_idx\n",
    "result_binary = np.argwhere(result == 0)\n",
    "correct = np.size(result_binary,0)\n",
    "acc = correct/test_lab_idx.shape[0]*100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57956a0c-8cc7-49c8-a774-60a6b7980a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### When using the top two PCs, we see a modest classification rate on the test set. However, using two PC's here is arbitrary and is easy simply due to visualization purposes. We can also exmaine accuracy as a function of the number of PCs and visualize the PC loading maps to interpret the classification results. But first, let's run some statistics to determine if the classification is significant in its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0231ff-6e49-44a1-80fe-ecdb19f148e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Permutation testing on shuffled data as a control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e627c742-c323-4e62-a12a-0a7999f5188c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parallelize time-consuming iterative processes with Dask (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3edf653-9c21-4f55-afb4-4c923fbacbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker = 4, n_workers = 1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c33448e8-f3ad-4c90-998b-b5c153fa3397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shuffle data labels in the training set when building RF classifier\n",
    "\n",
    "#### Do this a number of times to generate a null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997a48ef-a218-4eae-84ca-3ac95a3efac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "train_lab_idx_shuffle = train_lab_idx\n",
    "load_train = np.matmul(train_data, loadings[:,:2])\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def classification_RF_shuffle(load_train, train_lab_idx_shuffle, load_test, test_lab_idx):\n",
    "    \n",
    "    # Train classifier with new shuffled labels each iteration\n",
    "    rf_1 = RandomForestClassifier()\n",
    "    rng.shuffle(train_lab_idx_shuffle)\n",
    "    rf_1.fit(load_train, train_lab_idx_shuffle)\n",
    "\n",
    "    # Predict unseen data\n",
    "    predictions_1 = rf_1.predict(load_test)\n",
    "    test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "\n",
    "    # compare predictions with test labels and compute accuracy\n",
    "    result = predictions_1 - test_lab_idx\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary,0)\n",
    "    acc = correct/test_lab_idx.shape[0]*100\n",
    "    return acc\n",
    "\n",
    "# Parallelize lazy function to reduce computation time\n",
    "## limit number of iterations in this function to account for computer memory limit\n",
    "def iteration_permute(load_train, train_lab_idx_shuffle, load_test, test_lab_idx):\n",
    "    \n",
    "    acc_shuffle = []\n",
    "    for i in range(100):\n",
    "        acc_tmp = dask.delayed(classification_RF_shuffle)(load_train, train_lab_idx_shuffle, load_test, test_lab_idx)\n",
    "        acc_shuffle.append(acc_tmp)\n",
    "    \n",
    "    # compute values\n",
    "    acc_shuffle = dask.compute(*acc_shuffle)\n",
    "    acc_shuffle = np.array([float(num) for num in acc_shuffle])\n",
    "    clear_output(wait = True)\n",
    "    return acc_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40c9ebed-01df-4265-8b92-f8653547fc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Permute a sufficient number of times (here 500)\n",
    "\n",
    "### Not the most efficient way to do this, but must consider memory on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff9ea68-a750-4900-9c4d-e7585c81568b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc_shuffle_compile = []\n",
    "for i in range(5):\n",
    "    acc_tmp = iteration_permute(load_train, train_lab_idx_shuffle, load_test, test_lab_idx)\n",
    "    acc_shuffle_compile = np.concatenate((acc_shuffle_compile, acc_tmp), axis=0)\n",
    "\n",
    "b = plt.boxplot(acc_shuffle_compile)\n",
    "plt.title('Shuffled Classification Accuracy')\n",
    "plt.xlabel('Shuffled')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9dc29da-d673-4fb8-acd8-aa2af0a55009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Side note: validate parallel processing by comparing computation against serial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82399334-dca9-47cb-9154-fc90aab623f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# simple test to ensure that parallel processing is faster\n",
    "\n",
    "# traditional for loop\n",
    "acc_shuffle_nodask = []\n",
    "for i in range(100):\n",
    "    acc_tmp = classification_RF_shuffle(load_train, train_lab_idx, load_test, test_lab_idx)\n",
    "    acc_shuffle_nodask.append(acc_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a7aac85-4d87-4ea3-a06a-0f40e2a0370d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# dask parallelization\n",
    "acc_shuffle_dask = []\n",
    "for i in range(1):\n",
    "    acc_tmp = iteration_permute(load_train, train_lab_idx_shuffle, load_test, test_lab_idx)\n",
    "    acc_shuffle_dask = np.concatenate((acc_shuffle_dask, acc_tmp), axis=0)\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992562f1-cab6-4eae-b6e7-ae1c1e80b61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### We can in fact see that parallelization with dask does markedly reduce the computation time (on the CPU). Of course, IO data transfer is the bottleneck here, but this will reduce computation time enough for this example on a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed65ada2-3434-4240-9ebf-15e5459eb746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Determine significance of classification accuracy (permutation test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b286943-365b-4b97-9af6-ce36969a8127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_above_value = (acc_shuffle_compile > acc).sum()\n",
    "p_value = null_above_value/np.size(acc_shuffle_compile,0)\n",
    "print(p_value)\n",
    "\n",
    "# We can see here that the classification accuracy is not sigificcantly different from chance with p-value > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2141aa07-eac8-40c2-8133-d682e62ce9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Now explore RF classification as a function of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c789c247-0ed4-4d9e-be09-5860ea80ee95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def classification_RF_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, num_pc):\n",
    "\n",
    "    load_train = np.matmul(train_data, loadings[:,:num_pc])\n",
    "    \n",
    "    rf_1 = RandomForestClassifier()\n",
    "    rf_1.fit(load_train, train_lab_idx)\n",
    "    \n",
    "    load_test = np.matmul(test_data, loadings[:,:num_pc])\n",
    "    \n",
    "    # Predict unseen data\n",
    "    predictions_1 = rf_1.predict(load_test)\n",
    "    test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "    \n",
    "    # compare predictions with test labels and compute accuracy\n",
    "    result = predictions_1 - test_lab_idx\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary,0)\n",
    "    acc = correct/test_lab_idx.shape[0]*100\n",
    "    return acc\n",
    "\n",
    "def iteration_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, pc_idx):\n",
    "    \n",
    "    acc_PC = []\n",
    "    for i in pc_idx:\n",
    "        acc_tmp = dask.delayed(classification_RF_PCs)(train_data, train_lab_idx, test_data, test_lab_idx, loadings, i)\n",
    "        acc_PC.append(acc_tmp)\n",
    "    \n",
    "    # compute values\n",
    "    acc_PC = dask.compute(*acc_PC)\n",
    "    clear_output(wait = True)\n",
    "    acc_PC = np.array([float(num) for num in acc_PC])\n",
    "    return acc_PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262282e1-a9a7-494b-81c4-5283c1e762cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch parallel processing to conserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e82591-0f51-4c47-863f-9827730efe70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = [np.linspace(1,50,50, dtype = 'int'), np.linspace(51,100,50, dtype = 'int')]\n",
    "\n",
    "acc_PC_compile = []\n",
    "for i in range(len(idx)):\n",
    "    \n",
    "    acc_tmp = iteration_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, idx[i])\n",
    "    acc_PC_compile = np.concatenate((acc_PC_compile, acc_tmp), axis = 0)\n",
    "\n",
    "# plot accuracy as a function of PCs\n",
    "plt.plot(np.linspace(1,np.size(acc_PC_compile,0), np.size(acc_PC_compile,0)), acc_PC_compile, 'k')\n",
    "plt.title('Classification accuracy')\n",
    "plt.xlabel('# of PCs included')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 100)\n",
    "plt.xlim(-5, 105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad4e16b-cb8d-48f8-9478-6d5249e8591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Permutation testing for each number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df58d3a8-c84c-4118-9cc9-5265a27cfc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def classification_RF_shuffle_PCs(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, num_pc):\n",
    "\n",
    "    load_train = np.matmul(train_data, loadings[:,:num_pc])\n",
    "    \n",
    "    rf_1 = RandomForestClassifier()\n",
    "    rng.shuffle(train_lab_idx_shuffle)\n",
    "    rf_1.fit(load_train, train_lab_idx_shuffle)\n",
    "    \n",
    "    load_test = np.matmul(test_data, loadings[:,:num_pc])\n",
    "    \n",
    "    # Predict unseen data\n",
    "    predictions_1 = rf_1.predict(load_test)\n",
    "    test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "    \n",
    "    # compare predictions with test labels and compute accuracy\n",
    "    result = predictions_1 - test_lab_idx\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary,0)\n",
    "    acc = correct/test_lab_idx.shape[0]*100\n",
    "    return acc\n",
    "\n",
    "def iteration_permute_PCs(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, num_pc):\n",
    "    \n",
    "    acc_shuffle = []\n",
    "    for i in range(100):\n",
    "        acc_tmp = dask.delayed(classification_RF_shuffle_PCs)(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, num_pc)\n",
    "        acc_shuffle.append(acc_tmp)\n",
    "    \n",
    "    # compute values\n",
    "    acc_shuffle = dask.compute(*acc_shuffle)\n",
    "    acc_shuffle = np.array([float(num) for num in acc_shuffle])\n",
    "    clear_output(wait = True)\n",
    "    return acc_shuffle\n",
    "\n",
    "# def iteration_shuffle_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, pc_idx):\n",
    "    \n",
    "#     acc_PC = []\n",
    "#     for i in pc_idx:\n",
    "#         acc_tmp = dask.delayed(iteration_permute_PCs)(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, i)\n",
    "#         acc_PC.append(acc_tmp)\n",
    "    \n",
    "#     # compute values\n",
    "#     acc_PC = dask.compute(*acc_PC)\n",
    "#     clear_output(wait = True)\n",
    "#     # acc_PC = np.array([float(num) for num in acc_PC])\n",
    "#     return acc_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f274d29-3cec-4973-9e72-14e5cd9fca02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = np.linspace(1,100,100, dtype = 'int')\n",
    "acc_shuffle_PC_compile = []\n",
    "for i in idx:\n",
    "    acc_tmp = iteration_permute_PCs(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, i)\n",
    "    acc_shuffle_PC_compile.append(acc_tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd8e7e3-1952-4f19-ab0c-2b400ada08ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compare classification accuracy to chance level for each number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "573cd8bf-998b-44d1-8e34-ef5e59a0c44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot accuracy as a function of PCs\n",
    "\n",
    "# accuracy as a function of PCs\n",
    "plt.plot(np.linspace(1,np.size(acc_PC_compile,0), np.size(acc_PC_compile,0)), acc_PC_compile, 'k')\n",
    "\n",
    "# corresponding chance level\n",
    "p_value_compile = []\n",
    "for i in range(len(acc_shuffle_PC_compile)):\n",
    "    b = plt.boxplot(acc_shuffle_PC_compile[i], 'b', positions = [i+1], flierprops = {'marker': '.', 'markersize': 5})\n",
    "\n",
    "    # permutation testing\n",
    "    null_above_value = (acc_shuffle_PC_compile[i] > acc_PC_compile[i]).sum()\n",
    "    p_value_compile.append(null_above_value/np.size(acc_shuffle_PC_compile[i],0))\n",
    "\n",
    "# visualize statistical results\n",
    "p_value_compile = np.array([float(num) for num in p_value_compile])\n",
    "p_value_adj = false_discovery_control(p_value_compile)   \n",
    "\n",
    "for i in range(len(p_value_adj)): \n",
    "    if p_value_adj[i] < 0.05:\n",
    "        plt.scatter(i, 95, s = 10, c = 'r', marker = \"*\")\n",
    "\n",
    "plt.title('Classification accuracy')\n",
    "plt.xlabel('# of PCs included')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 100)\n",
    "plt.xlim(-5, 105)\n",
    "p_value_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b1a48c-0b1a-4ca9-add8-38b42a158d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature interpretation\n",
    "\n",
    "### Visualize spatial PC maps to identify features that contribute to data variance and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd55ec8c-70a6-466c-9843-f56fc99e2806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(5,4)\n",
    "lim = 20\n",
    "for ipc in range(lim):\n",
    "\n",
    "    PC_map = loadings[:,ipc]\n",
    "    PC_map = np.reshape(PC_map, (128,128))\n",
    "    clim = max(abs(np.min(PC_map)), abs(np.max(PC_map)))\n",
    "\n",
    "    idx1 = np.floor(ipc/4).astype(int)\n",
    "    idx2 = np.fmod(ipc,4)\n",
    "    ax[idx1, idx2].axis('off')\n",
    "    ax[idx1, idx2].imshow(PC_map, cmap = \"magma\", vmin = -clim, vmax = clim)\n",
    "    ax[idx1, idx2].set_title(f'PC# {ipc +1}', fontsize = 10)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f923ef6c-0aab-44d9-9bf9-58fc7705d88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Here we can see that for each of the top 20 PC's there are brain regions are more strongly weighted than others (in either the positive or negative direction). It is important to note that it does not appear that each image used in this dataset came from the same location/slice within the brain. We do not know if certain slices are over- or under-represented in different AD classes and therefore cannot say with certainty whether specific anatomical structures drive different stages of AD. Nevertheless,"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_random_forest",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
