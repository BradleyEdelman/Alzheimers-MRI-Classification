{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24cda55f-4c93-4a9b-93f1-46e40160dc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Alzheimer's Disease classification from anatomical MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977baf45-0422-4abc-aea2-e10e685469b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook explores the use of a custom CNN to classify Alzheimer's disease from anatomical MRI images.\n",
    "\n",
    "Briefly, the pipeline involves the following steps and technical features:\n",
    "\n",
    "- Data formating and quality check\n",
    "- Custom CNN for classification\n",
    "- Hyperparameter tuning\n",
    "- Final model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2c498f-af11-4e5a-844f-7137a3c7b8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning and statistics\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import plot_model\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.stats import false_discovery_control\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel computing\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import cv2, magic, datetime, sys, os, wget\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Workspace/Users/bjedelma@gmail.com/Alzheimers-MRI-Classification/src')\n",
    "from img_preprocessing import dict_to_image\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61681ce4-97d0-4fb5-823f-5d1658878c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mount AWS S3 bucket containing parquet data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ae2830-cddd-47f6-84de-ae8053c8f050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "AWS_S3_BUCKET = \"databricks-workspace-stack-brad-personal-bucket/AD_MRI_classification/\"\n",
    "KEY_FILE = \"/FileStore/tables/brad_databricks_personal_accessKeys_new.csv\"\n",
    "\n",
    "# extract aws credentials from hidden table \n",
    "aws_keys_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \",\").load(KEY_FILE)\n",
    "\n",
    "ACCESS_KEY = aws_keys_df.collect()[0][0]\n",
    "SECRET_KEY = aws_keys_df.collect()[0][1]\n",
    "\n",
    "# specify bucket and mount point\n",
    "MOUNT_NAME = f\"/mnt/{AWS_S3_BUCKET.split('/')[-2]}\"\n",
    "SOURCE_URL = f\"s3a://{AWS_S3_BUCKET}\"\n",
    "EXTRA_CONFIGS = { \"fs.s3a.access.key\": ACCESS_KEY, \"fs.s3a.secret.key\": SECRET_KEY}\n",
    "\n",
    "# mount bucket\n",
    "if any(mount.mountPoint == MOUNT_NAME for mount in dbutils.fs.mounts()):\n",
    "    print(f\"{MOUNT_NAME} is already mounted.\")\n",
    "else:\n",
    "    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME, extra_configs = EXTRA_CONFIGS)\n",
    "    print(f\"{MOUNT_NAME} is now mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04dd10a8-8bb0-451c-bdc6-4cfca23373dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom CNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f86d26d-3d1c-41b3-b193-d2d11e805d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # three convolutional layers and one fully connected layer\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape = (128, 128, 1)),\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation = 'relu'), # fully connected layer\n",
    "        keras.layers.Dense(4, activation = 'softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb9b299-53e0-47b7-89bb-a6e8ca8f51cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd3187-19e9-42dd-b5ca-6b7013c4d857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_lab = to_categorical(train_lab_idx.astype('int8'))\n",
    "history = model.fit(train_data, train_lab, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb7b2eba-8d0b-4218-a6c1-96bed9788ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### visualize model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f94e7-41a1-43f9-92fe-f7d45f6f5f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color='tab:red')\n",
    "ax1.plot(history.history['loss'], 'r', label='Loss')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy (%)', color='tab:blue')\n",
    "ax2.plot(np.array(history.history['accuracy']) * 100, 'k', label='Accuracy (%)')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9245568-f715-434e-be95-e2d3a87e4440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413bc7e3-782f-44a1-b9f5-ff6fe5cec93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob = model.predict(test_data)\n",
    "predict_classes=np.argmax(prob,axis=1)\n",
    "predict_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a3dd36-4b6b-4cc3-a156-ac58a196755c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Evaluate accuracy and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b6fdff-df96-40d8-b0f3-1f96ef933549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "conf_matrix = confusion_matrix(test_lab_idx, predict_classes)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', cbar_kws={'label': 'Accuracy (%)'}, \n",
    "            xticklabels=['No AD', 'Mild AD', 'Moderate AD', 'Severe AD'], \n",
    "            yticklabels=['No AD', 'Mild AD', 'Moderate AD', 'Severe AD'], \n",
    "            ax=ax[0], linewidths=1, linecolor='black')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "ax[0].set_ylabel('True Labels')\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "\n",
    "# Bar plot for the distribution of the test set\n",
    "train_label_counts = train['label'].value_counts().sort_index()\n",
    "ax[1].bar(train_label_counts.index, train_label_counts.values, color = ['#aec7e8', '#ffbb78', '#98df8a', '#ff9896'])\n",
    "ax[1].set_xlabel('Class Label')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Distribution of Training Set')\n",
    "ax[1].set_xticks(train_label_counts.index)\n",
    "ax[1].set_xticklabels(['No AD', 'Mild AD', 'Moderate AD', 'Severe AD'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "320a99a1-05db-4808-b83e-dfdd3e17351f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Overall we do not see a direct link between the test set class accuracies and the number of measurements in the training set. In fact the class with the lowest number of training measurements achieved the second highest accuracy. Therefore, even though it would be ideal to have completely (or moreso) balanced training classes, this is not a major bias in the current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59f3f73-75d6-4f65-be37-7aced852f850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e6dc37-2f01-4b72-b679-ae581e05daa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define parameters to tune and the corresponding parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8608ee18-9205-4820-b536-75af25024159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same architecture as before, but with hyperparameter ranges\n",
    "def build_model(hp):\n",
    "\n",
    "    model = keras.Sequential([\n",
    "    keras.Input(shape = (128, 128, 1)),  \n",
    "        \n",
    "    keras.layers.Conv2D(\n",
    "        filters = hp.Int('conv_1_filter', min_value = 32, max_value = 128, step = 32), \n",
    "        kernel_size = hp.Choice('conv_1_kernel', values = [3,3]), \n",
    "        activation = 'relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)), \n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    keras.layers.Conv2D(\n",
    "        filters = hp.Int('conv_2_filter', min_value = 64, max_value = 128, step = 32),\n",
    "        kernel_size = hp.Choice('conv_2_kernel', values = [3,3]),\n",
    "        activation = 'relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)), \n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    keras.layers.Conv2D(\n",
    "        filters = hp.Int('conv_3_filter', min_value = 96, max_value = 128, step = 32),\n",
    "        kernel_size = hp.Choice('conv_3_kernel', values = [3,3]),\n",
    "        activation = 'relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(\n",
    "        units=hp.Int('dense_1_units', min_value = 128, max_value = 256, step = 32),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        \n",
    "    keras.layers.Dropout(0.5),\n",
    "        \n",
    "    keras.layers.Dense(4, activation = 'softmax')\n",
    "    ])\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0878966c-e83a-41a9-8ff9-705a106134ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initiate tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac57726-1842-4130-9822-e61f8c18bb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model, objective = 'val_accuracy', max_epochs = 10, factor = 3, directory = 'my_dir', project_name = 'AD_class')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3295b515-54bc-4fa6-8c40-75cb62974a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37108826-ab82-47cf-8c5d-9b749b1c9ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need numpy arrays rather than tensors for tuner\n",
    "train_images = np.asarray(list(train_ds.map(lambda x, y: x)))\n",
    "train_labels = np.asarray(list(train_ds.map(lambda x, y: y)))\n",
    "train_labels = to_categorical(train_labels.astype('int8'))\n",
    "\n",
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_lab_idx))\n",
    "test_images = np.asarray(list(test_ds.map(lambda x, y: x)))\n",
    "test_labels = np.asarray(list(test_ds.map(lambda x, y: y)))\n",
    "test_labels = to_categorical(test_labels.astype('int8'))\n",
    "\n",
    "tuner.search(train_images, train_labels, epochs = 10, callbacks = [stop_early],\n",
    "             validation_data = (test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575758c9-845b-4566-a614-c5be32f31395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal parameters are as follows:\n",
    "\n",
    "Filter 1 output dim: {best_hps.get('conv_1_filter')}\n",
    "Filter 2 output dim: {best_hps.get('conv_2_filter')}\n",
    "Filter 2 output dim: {best_hps.get('conv_3_filter')}\n",
    "\n",
    "Dense layer units: {best_hps.get('units')}\n",
    "\n",
    "Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c884d7e-f94a-435e-b3a2-3f14b6b3a660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optimal # of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e531b7da-bcdc-40f3-a3f6-278cc7fb8b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model = tuner.hypermodel.build(best_hps)\n",
    "# history = model.fit(train_images, train_labels, epochs = 50, validation_split = 0.2)\n",
    "\n",
    "# val_acc_per_epoch = history.history['val_accuracy']\n",
    "# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "# print('Best epoch: %d' % (best_epoch,))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_custom_CNN",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
