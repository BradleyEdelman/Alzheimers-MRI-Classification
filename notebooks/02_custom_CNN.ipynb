{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24cda55f-4c93-4a9b-93f1-46e40160dc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Alzheimer's Disease classification from anatomical MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977baf45-0422-4abc-aea2-e10e685469b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook explores the use of a custom CNN to classify Alzheimer's disease from anatomical MRI images.\n",
    "\n",
    "Briefly, the pipeline involves the following steps and technical features:\n",
    "\n",
    "- Data formating and quality check\n",
    "- Custom CNN for classification\n",
    "- Hyperparameter tuning\n",
    "- Final model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2c498f-af11-4e5a-844f-7137a3c7b8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning and statistics\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import plot_model\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.stats import false_discovery_control\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel computing\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import cv2, magic, datetime, sys, os, wget\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Download the file to a local path\n",
    "local_path = '/tmp/helpers.py'\n",
    "wget.download('https://raw.githubusercontent.com/BradleyEdelman/Alzheimers-MRI-Classification/main/Databricks/functions/Distributed_strategy.py', local_path)\n",
    "dbutils.fs.cp(f'file:{local_path}', 'dbfs:/tmp/helpers.py', True)\n",
    "sys.path.append('/dbfs/tmp/')\n",
    "import helpers\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cca85d-bc65-4d28-be12-4c778223522f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strategy = helpers5.distributed_strategy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61681ce4-97d0-4fb5-823f-5d1658878c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mount AWS S3 bucket containing parquet data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ae2830-cddd-47f6-84de-ae8053c8f050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "AWS_S3_BUCKET = \"databricks-workspace-stack-brad-personal-bucket/AD_MRI_classification/\"\n",
    "KEY_FILE = \"/FileStore/tables/brad_databricks_personal_accessKeys_new.csv\"\n",
    "\n",
    "# extract aws credentials from hidden table \n",
    "aws_keys_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \",\").load(KEY_FILE)\n",
    "\n",
    "ACCESS_KEY = aws_keys_df.collect()[0][0]\n",
    "SECRET_KEY = aws_keys_df.collect()[0][1]\n",
    "\n",
    "# specify bucket and mount point\n",
    "MOUNT_NAME = f\"/mnt/{AWS_S3_BUCKET.split('/')[-2]}\"\n",
    "SOURCE_URL = f\"s3a://{AWS_S3_BUCKET}\"\n",
    "EXTRA_CONFIGS = { \"fs.s3a.access.key\": ACCESS_KEY, \"fs.s3a.secret.key\": SECRET_KEY}\n",
    "\n",
    "# mount bucket\n",
    "if any(mount.mountPoint == MOUNT_NAME for mount in dbutils.fs.mounts()):\n",
    "    print(f\"{MOUNT_NAME} is already mounted.\")\n",
    "else:\n",
    "    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME, extra_configs = EXTRA_CONFIGS)\n",
    "    print(f\"{MOUNT_NAME} is now mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05789582-02fb-4576-b3fb-abd4638c885c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import analysis and plotting libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e115089e-56cd-4dcc-b762-1986a79bd547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and format training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c07fac65-2da1-4fe0-a14d-18eef031feb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Label meanings\n",
    "\n",
    "- 0 - Mild dementia\n",
    "- 1 - Moderate dementia\n",
    "- 2 - No dementia\n",
    "- 3 - Very mild dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd55f3a-e85a-4051-a447-132dec2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Lab = ['Mild', 'Moderate', 'None', 'Very Mild']\n",
    "\n",
    "train = pd.read_parquet(\"/dbfs/mnt/AD_classification/train-00000-of-00001-c08a401c53fe5312.parquet\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270fb0bd-fd17-49ee-95cc-5bf13abc8020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Convert data to readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2722547d-cfce-4fc1-8362-224446154a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dict_to_image(image_dict):\n",
    "    if isinstance(image_dict, dict) and 'bytes' in image_dict:\n",
    "        byte_string = image_dict['bytes']\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
    "        return img\n",
    "    else:\n",
    "        raise TypeError(f\"Expected dictionary with 'bytes' key, got {type(image_dict)}\")\n",
    "\n",
    "train['img_arr'] = train['image'].apply(dict_to_image)\n",
    "train.drop(\"image\", axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79316728-c086-4842-8454-253a1d6f8603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and format test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33fed32-016a-4778-8274-95d3425ab3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"/dbfs/mnt/AD_classification/test-00000-of-00001-44110b9df98c5585.parquet\")\n",
    "\n",
    "test['img_arr'] = test['image'].apply(dict_to_image)\n",
    "test.drop(\"image\", axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a24a7f0e-fbfc-4a51-8806-6779cf80a827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore structure and visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fde75e86-4688-4be3-820d-0b5ee8168b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distribution of the datasets (are all classes represented equally?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2acaa56-1500-4ef1-a1e4-59d8048c45a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['#aec7e8', '#ffbb78', '#98df8a', '#ff9896']\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "unique, counts = np.unique(np.asarray(train.iloc[:].label), return_counts=True)\n",
    "ax[0].bar(unique, counts, color=colors)\n",
    "ax[0].set_xticks(unique)\n",
    "ax[0].set_xticklabels(Lab, rotation=45)\n",
    "ax[0].set_title('Training')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('# of images')\n",
    "\n",
    "unique, counts = np.unique(np.asarray(test.iloc[:].label), return_counts=True)\n",
    "ax[1].bar(unique, counts, color=colors)\n",
    "ax[1].set_xticks(unique)\n",
    "ax[1].set_xticklabels(Lab, rotation=45)\n",
    "ax[1].set_title('Testing')\n",
    "ax[1].set_xlabel('Class')\n",
    "ax[1].set_ylabel('# of images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7a0473-41d5-4581-884a-c035bd561858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see that there is an obvious imbalance across classes in both the training and testing sets. However, each class has been proportionally split between the two. Nevertheless, let's attempt to balance the training set such that the model sees equal numbers of each class. To avoid overfitting (e.g. simple resampling), we use the SMOTE method here to synthetically generate new data based on what is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab80d8a-fb77-4992-90ca-87bbd7fc8086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SMOTE: Synthetic Minority Over-Sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7aacda-8c11-48e2-b5b6-d6060eeda7db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X_train = np.array([img.flatten() for img in train['img_arr']])\n",
    "y_train = train['label']\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train.reshape(-1, 128*128), y_train)\n",
    "\n",
    "# Reshape the resampled data back to the original image shape\n",
    "X_resampled = X_resampled.reshape(-1, 128, 128)\n",
    "\n",
    "# Create a new DataFrame with the resampled data\n",
    "train_resampled = pd.DataFrame({'label': y_resampled, 'img_arr': [img.tolist() for img in X_resampled]})\n",
    "\n",
    "# Extract labels from the resampled training data\n",
    "y_resampled = train_resampled['label']\n",
    "\n",
    "# Plot the distribution of the different classes\n",
    "colors = ['#aec7e8', '#ffbb78', '#98df8a', '#ff9896']\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "ax.bar(unique, counts, color=colors)\n",
    "ax.set_xticks(unique)\n",
    "ax.set_xticklabels(Lab, rotation=45)\n",
    "ax.set_title('Resampled Training')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('# of images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9efef3c-3652-4568-8847-b357a37deabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we see that the training set is balanced across classes. Let's inspect some of the new data for quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dac3c24-97c2-4cb7-8f44-d8b220e5eade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "first_indices = {}\n",
    "for num in range(0, 4):\n",
    "    first_index = next((i for i, x in enumerate(y_resampled[5121:], start=5121) if x == num), None)\n",
    "    first_indices[num] = first_index\n",
    "\n",
    "print(first_indices)\n",
    "\n",
    "# Visualize the images from the first_indices values\n",
    "f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i, label in enumerate(np.unique(y_resampled)):\n",
    "    if first_indices[label] is not None:\n",
    "        first_image = np.array(train_resampled.iloc[first_indices[label]]['img_arr']).reshape(128, 128)\n",
    "        ax[i].imshow(first_image, cmap='gray')\n",
    "        ax[i].set_title(f\"{Lab[label]}: SMOTE\")\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d8632e-4038-41ea-8d74-7049a1eadaa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b022ed5-b189-4946-b4f9-c23f3758fba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Visually inspect original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf84c61-ac2e-42ac-84f2-8bd34c4a2b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lab_idx = np.asarray(train.iloc[:].label)\n",
    "\n",
    "f, ax = plt.subplots(4,4)\n",
    "for lab in range(4):\n",
    "    for ex in range(4):\n",
    "    \n",
    "        class_lab = np.argwhere(train_lab_idx == 1)\n",
    "        current_idx = np.random.randint(len(class_lab)-1,size = 1)\n",
    "        current_idx = np.asarray(current_idx)\n",
    "        \n",
    "        ax[ex, lab].axis('off')\n",
    "        ax[ex, lab].imshow(train.iloc[class_lab[current_idx[0]][0]].img_arr, cmap = \"gray\")\n",
    "        if ex == 0: ax[ex, lab].set_title(Lab[lab])\n",
    "\n",
    "# Clearly, images show different slices within the brain, which may be a major confound..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "536fb1c0-a654-40e8-8096-afc40afb5ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format images (stored in local memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55334cca-abd1-4496-acdf-15beb21d38f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "train_data = np.empty((len(train), 128, 128))\n",
    "for i in range(len(train)):\n",
    "   train_data[i, :, :] = train.iloc[i].img_arr\n",
    "\n",
    "# test data\n",
    "test_data = np.empty((len(test), 128, 128))\n",
    "for i in range(len(test)):\n",
    "   test_data[i, :, :] = test.iloc[i].img_arr\n",
    "\n",
    "# format in 3D shape that keras likes\n",
    "train_data = np.expand_dims(train_data, axis = 3)\n",
    "test_data = np.expand_dims(test_data, axis = 3)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2221ceee-990d-4ff3-81d4-bdf7d2faffb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8537629-238b-42f3-bdab-8f6c2a877f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Resize and rescale functions\n",
    "IMG_SIZE = 128\n",
    "resize_and_rescale = keras.Sequential([\n",
    "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepare(ds, shuffle = False, augment =False):\n",
    "  # Resize and rescale all datasets.\n",
    "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y),\n",
    "              num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(1000)\n",
    "\n",
    "  # Batch all datasets.\n",
    "  ds = ds.batch(batch_size)\n",
    "\n",
    "  # Use data augmentation only on the training set.\n",
    "  if augment:\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training = True), y),\n",
    "                num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "  # Use buffered prefetching on all datasets.\n",
    "  return ds.prefetch(buffer_size = AUTOTUNE)\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f58127-622f-43c2-9d13-7d7a7c858e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Augment training dataset only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8721c3b-a148-485e-af59-cb634f6daeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify augmentation on first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2eb695-46a3-426e-92a2-e9e101bd1ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample dataset\n",
    "train_data_tmp = train_data[0]\n",
    "train_data_tmp = np.expand_dims(train_data_tmp, axis = 0)\n",
    "\n",
    "# apply augmentation\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_data_tmp, train_lab_idx[:1]))\n",
    "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "\n",
    "# view augmented images\n",
    "train_ds = train_ds.unbatch()\n",
    "images = np.asarray(list(train_ds.map(lambda x, y: x)))\n",
    "clear_output(wait=False)\n",
    "\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].axis('off')\n",
    "ax[0].imshow(train_data[0], cmap = \"gray\",)\n",
    "ax[0].set_title('Original', fontsize = 10)\n",
    "\n",
    "ax[1].axis('off')\n",
    "ax[1].imshow(images[0,:,:,:], cmap = \"gray\",)\n",
    "ax[1].set_title('Augmented', fontsize = 10)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93c11cfb-9e34-42c5-81cc-336c4d82c8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Augment multiple times to generate larger training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbaee713-c6cf-4e21-b2cf-b4d6081193ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'your-bucket-name'\n",
    "file_key = 'CNN - train_data_augment'\n",
    "\n",
    "try:\n",
    "    s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "    file_exists = True\n",
    "except:\n",
    "    file_exists = False\n",
    "\n",
    "if not file_exists:\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_lab_idx))\n",
    "    train_ds_for_aug = train_ds\n",
    "\n",
    "    for i in range(1):\n",
    "        train_aug = prepare(train_ds_for_aug, shuffle=True, augment=True)\n",
    "        train_aug = train_aug.unbatch()\n",
    "        train_aug = train_aug.map(lambda x, y: (tf.cast(x, tf.float64), y))\n",
    "        train_ds = train_ds.concatenate(train_aug)\n",
    "\n",
    "    del train_aug, train_ds_for_aug\n",
    "    images = np.asarray(list(train_ds.map(lambda x, y: x)))\n",
    "    labels = np.asarray(list(train_ds.map(lambda x, y: y)))\n",
    "    clear_output(wait=False)\n",
    "    print(images.shape)\n",
    "\n",
    "    np.savez('/dbfs/tmp/train_data_augment.npz', images=images, labels=labels)\n",
    "    s3.upload_file('/dbfs/tmp/train_data_augment.npz', bucket_name, file_key)\n",
    "else:\n",
    "    local_file_path = '/dbfs/tmp/train_data_augment.npz'\n",
    "    s3.download_file(bucket_name, file_key, local_file_path)\n",
    "    data = np.load(local_file_path)\n",
    "    images = data['images']\n",
    "    labels = data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04dd10a8-8bb0-451c-bdc6-4cfca23373dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom CNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f86d26d-3d1c-41b3-b193-d2d11e805d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# three convolutional layers and one fully connected layer\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape = (128, 128, 1)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation = 'relu'), # fully connected layer\n",
    "    keras.layers.Dense(4, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6248d0bb-aca8-47bc-be4d-7760aa543ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750743ad-fb12-42ec-9249-8c75b07b5141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb9b299-53e0-47b7-89bb-a6e8ca8f51cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd3187-19e9-42dd-b5ca-6b7013c4d857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_log_dir = \"/dbfs/FileStore/logs/\"\n",
    "run_log_dir = experiment_log_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "train_lab = to_categorical(train_lab_idx.astype('int8'))\n",
    "history = model.fit(train_data, train_lab, epochs=20, batch_size=32, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e0069e-7dbd-4db6-960f-42f7495f0984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir $run_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb7b2eba-8d0b-4218-a6c1-96bed9788ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### visualize model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0f94e7-41a1-43f9-92fe-f7d45f6f5f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color='tab:red')\n",
    "ax1.plot(history.history['loss'], 'r', label='Loss')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy (%)', color='tab:blue')\n",
    "ax2.plot(np.array(history.history['accuracy']) * 100, 'k', label='Accuracy (%)')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9245568-f715-434e-be95-e2d3a87e4440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413bc7e3-782f-44a1-b9f5-ff6fe5cec93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob = model.predict(test_data)\n",
    "predict_classes=np.argmax(prob,axis=1)\n",
    "predict_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a3dd36-4b6b-4cc3-a156-ac58a196755c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Evaluate accuracy and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b6fdff-df96-40d8-b0f3-1f96ef933549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "conf_matrix = confusion_matrix(test_lab_idx, predict_classes)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', cbar_kws={'label': 'Accuracy (%)'}, \n",
    "            xticklabels=['No AD', 'Mild AD', 'Moderate AD', 'Severe AD'], \n",
    "            yticklabels=['No AD', 'Mild AD', 'Moderate AD', 'Severe AD'], \n",
    "            ax=ax[0], linewidths=1, linecolor='black')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "ax[0].set_ylabel('True Labels')\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "\n",
    "# Bar plot for the distribution of the test set\n",
    "train_label_counts = train['label'].value_counts().sort_index()\n",
    "ax[1].bar(train_label_counts.index, train_label_counts.values, color = ['#aec7e8', '#ffbb78', '#98df8a', '#ff9896'])\n",
    "ax[1].set_xlabel('Class Label')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Distribution of Training Set')\n",
    "ax[1].set_xticks(train_label_counts.index)\n",
    "ax[1].set_xticklabels(['No AD', 'Mild AD', 'Moderate AD', 'Severe AD'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "320a99a1-05db-4808-b83e-dfdd3e17351f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Overall we do not see a direct link between the test set class accuracies and the number of measurements in the training set. In fact the class with the lowest number of training measurements achieved the second highest accuracy. Therefore, even though it would be ideal to have completely (or moreso) balanced training classes, this is not a major bias in the current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59f3f73-75d6-4f65-be37-7aced852f850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e6dc37-2f01-4b72-b679-ae581e05daa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define parameters to tune and the corresponding parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8608ee18-9205-4820-b536-75af25024159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same architecture as before, but with hyperparameter ranges\n",
    "def build_model(hp):\n",
    "\n",
    "    model = keras.Sequential([\n",
    "    keras.Input(shape = (128, 128, 1)),  \n",
    "        \n",
    "    keras.layers.Conv2D(\n",
    "        filters = hp.Int('conv_1_filter', min_value = 32, max_value = 128, step = 32), \n",
    "        kernel_size = hp.Choice('conv_1_kernel', values = [3,3]), \n",
    "        activation = 'relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)), \n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    keras.layers.Conv2D(\n",
    "        filters = hp.Int('conv_2_filter', min_value = 64, max_value = 128, step = 32),\n",
    "        kernel_size = hp.Choice('conv_2_kernel', values = [3,3]),\n",
    "        activation = 'relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)), \n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    keras.layers.Conv2D(\n",
    "        filters = hp.Int('conv_3_filter', min_value = 96, max_value = 128, step = 32),\n",
    "        kernel_size = hp.Choice('conv_3_kernel', values = [3,3]),\n",
    "        activation = 'relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(\n",
    "        units=hp.Int('dense_1_units', min_value = 128, max_value = 256, step = 32),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        \n",
    "    keras.layers.Dropout(0.5),\n",
    "        \n",
    "    keras.layers.Dense(4, activation = 'softmax')\n",
    "    ])\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0878966c-e83a-41a9-8ff9-705a106134ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initiate tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac57726-1842-4130-9822-e61f8c18bb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model, objective = 'val_accuracy', max_epochs = 10, factor = 3, directory = 'my_dir', project_name = 'AD_class')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3295b515-54bc-4fa6-8c40-75cb62974a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37108826-ab82-47cf-8c5d-9b749b1c9ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need numpy arrays rather than tensors for tuner\n",
    "train_images = np.asarray(list(train_ds.map(lambda x, y: x)))\n",
    "train_labels = np.asarray(list(train_ds.map(lambda x, y: y)))\n",
    "train_labels = to_categorical(train_labels.astype('int8'))\n",
    "\n",
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_lab_idx))\n",
    "test_images = np.asarray(list(test_ds.map(lambda x, y: x)))\n",
    "test_labels = np.asarray(list(test_ds.map(lambda x, y: y)))\n",
    "test_labels = to_categorical(test_labels.astype('int8'))\n",
    "\n",
    "tuner.search(train_images, train_labels, epochs = 10, callbacks = [stop_early],\n",
    "             validation_data = (test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575758c9-845b-4566-a614-c5be32f31395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal parameters are as follows:\n",
    "\n",
    "Filter 1 output dim: {best_hps.get('conv_1_filter')}\n",
    "Filter 2 output dim: {best_hps.get('conv_2_filter')}\n",
    "Filter 2 output dim: {best_hps.get('conv_3_filter')}\n",
    "\n",
    "Dense layer units: {best_hps.get('units')}\n",
    "\n",
    "Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c884d7e-f94a-435e-b3a2-3f14b6b3a660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optimal # of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e531b7da-bcdc-40f3-a3f6-278cc7fb8b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model = tuner.hypermodel.build(best_hps)\n",
    "# history = model.fit(train_images, train_labels, epochs = 50, validation_split = 0.2)\n",
    "\n",
    "# val_acc_per_epoch = history.history['val_accuracy']\n",
    "# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "# print('Best epoch: %d' % (best_epoch,))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_custom_CNN",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
