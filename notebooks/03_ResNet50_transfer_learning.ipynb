{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce36b5b0-e202-442f-bc72-c373085f43d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Alzheimer's Disease classification from anatomical MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee3eca32-5942-4c10-a19f-84e17aad6c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook explores the use of transfer learning with ResNet50 to classify Alzheimer's disease from anatomical MRI images.\n",
    "\n",
    "Briefly, the pipeline involves the following steps and technical features:\n",
    "\n",
    "- Data formating and quality check\n",
    "- Transfer learning using ResNet50\n",
    "- Hyperparameter tuning\n",
    "- Final model application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "849d4204-28eb-4075-9b98-d440d01601c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import analysis and plotting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018f7e13-4633-4a51-a4d1-25cdd08d7c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# extract aws credentials from hidden table \n",
    "aws_keys_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \",\").load(\"/FileStore/tables/brad_databricks_personal_accessKeys_new.csv\")\n",
    "\n",
    "ACCESS_KEY = aws_keys_df.collect()[0][0]\n",
    "SECRET_KEY = aws_keys_df.collect()[0][1]\n",
    "\n",
    "# specify bucket and mount point\n",
    "AWS_S3_BUCKET = \"databricks-workspace-stack-brad-personal-bucket/AD_MRI_classification/\"\n",
    "MOUNT_NAME = \"/mnt/AD_classification\"\n",
    "SOURCE_URL = f\"s3a://{AWS_S3_BUCKET}\"\n",
    "EXTRA_CONFIGS = { \"fs.s3a.access.key\": ACCESS_KEY, \"fs.s3a.secret.key\": SECRET_KEY}\n",
    "\n",
    "# mount bucket\n",
    "# dbutils.fs.unmount(MOUNT_NAME)\n",
    "# dbutils.fs.mount(SOURCE_URL, MOUNT_NAME, extra_configs = EXTRA_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ade3180-eaee-420f-b1b2-829033f85097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning and statistics\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras import applications, layers, models, applications, callbacks, optimizers\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.stats import false_discovery_control\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel computing\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import cv2\n",
    "import magic\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79480520-9264-4323-98e4-c8186e414e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and format training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379ee4c7-34e6-4e65-a56d-da4eb79c2d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Lab = ['Mild', 'Moderate', 'None', 'Very Mild']\n",
    "\n",
    "train = pd.read_parquet(\"/dbfs/mnt/AD_classification/train-00000-of-00001-c08a401c53fe5312.parquet\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1b62510-bcd8-4d0b-ad1e-3484439ef73d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Convert data to readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db63d3e-50d5-4091-8e87-e794581384f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dict_to_image(image_dict):\n",
    "    if isinstance(image_dict, dict) and 'bytes' in image_dict:\n",
    "        byte_string = image_dict['bytes']\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
    "        return img\n",
    "    else:\n",
    "        raise TypeError(f\"Expected dictionary with 'bytes' key, got {type(image_dict)}\")\n",
    "\n",
    "train['img_arr'] = train['image'].apply(dict_to_image)\n",
    "train.drop(\"image\", axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb42a7f7-e9dd-43d8-8420-9bd58ecb8aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and format test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8239bbc-541b-4e7a-a2cc-99b504cbb143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"/dbfs/mnt/AD_classification/test-00000-of-00001-44110b9df98c5585.parquet\")\n",
    "\n",
    "test['img_arr'] = test['image'].apply(dict_to_image)\n",
    "test.drop(\"image\", axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee7c9d4-6a6f-47a7-877a-77bd7f480cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore structure and visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "177c9881-dc98-4ef6-a5e0-9f2a0a6bd199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distribution of the datasets (are all classes represented equally?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b174e6-ed43-4a51-9212-83cc125b3e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['#aec7e8', '#ffbb78', '#98df8a', '#ff9896']\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "unique, counts = np.unique(np.asarray(train.iloc[:].label), return_counts=True)\n",
    "ax[0].bar(unique, counts, color=colors)\n",
    "ax[0].set_xticks(unique)\n",
    "ax[0].set_xticklabels(Lab, rotation=45)\n",
    "ax[0].set_title('Training')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('# of images')\n",
    "\n",
    "unique, counts = np.unique(np.asarray(test.iloc[:].label), return_counts=True)\n",
    "ax[1].bar(unique, counts, color=colors)\n",
    "ax[1].set_xticks(unique)\n",
    "ax[1].set_xticklabels(Lab, rotation=45)\n",
    "ax[1].set_title('Testing')\n",
    "ax[1].set_xlabel('Class')\n",
    "ax[1].set_ylabel('# of images')\n",
    "\n",
    "# An obvious imbalance across classes, but each class seems to be balanced across training/testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1895f05-bef2-4640-9e45-269b22a82ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visually inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47395522-030b-4d1e-a7d2-efa02942d528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lab_idx = np.asarray(train.iloc[:].label)\n",
    "\n",
    "f, ax = plt.subplots(4,4)\n",
    "for lab in range(4):\n",
    "    for ex in range(4):\n",
    "    \n",
    "        class_lab = np.argwhere(train_lab_idx == 1)\n",
    "        current_idx = np.random.randint(len(class_lab)-1,size = 1)\n",
    "        current_idx = np.asarray(current_idx)\n",
    "        \n",
    "        ax[ex, lab].axis('off')\n",
    "        ax[ex, lab].imshow(train.iloc[class_lab[current_idx[0]][0]].img_arr, cmap = \"gray\")\n",
    "        if ex == 0: ax[ex, lab].set_title(Lab[lab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f8c2c9-9522-4139-b9a6-02d8c1e06d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and adapt ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07dfb9b-f6a4-44a7-a020-ab6414276f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format images (stored in local memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ebf4af0-1a68-4c87-8436-ccc3e71f07e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "train_data = np.empty((len(train), 128, 128))\n",
    "for i in range(len(train)):\n",
    "   train_data[i, :, :] = train.iloc[i].img_arr\n",
    "\n",
    "# test data\n",
    "test_data = np.empty((len(test), 128, 128))\n",
    "for i in range(len(test)):\n",
    "   test_data[i, :, :] = test.iloc[i].img_arr\n",
    "\n",
    "# format in 3D shape that keras likes\n",
    "train_data = np.expand_dims(train_data, axis = 3)\n",
    "test_data = np.expand_dims(test_data, axis = 3)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96559d2a-a221-4737-929b-384af19144b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc4f76b-e87f-4f28-a798-241ad9c97c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Resize and rescale functions\n",
    "IMG_SIZE = 128\n",
    "resize_and_rescale = keras.Sequential([\n",
    "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "  # Resize and rescale all datasets.\n",
    "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y),\n",
    "              num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(1000, reshuffle_each_iteration=True)\n",
    "\n",
    "  # Batch all datasets.\n",
    "  ds = ds.batch(batch_size)\n",
    "\n",
    "  # Use data augmentation only on the training set.\n",
    "  if augment:\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "  # Use buffered prefetching on all datasets.\n",
    "  return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ecccb6-6fa6-4dbc-b87e-cbc7f8596a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Augment training dataset only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259c475b-83b1-4298-b2b1-44819d7683d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Augment multiple times to generate larger training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76df4125-1337-4fe8-b586-fdbfef4df649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_lab_idx))\n",
    "train_ds_for_aug = train_ds\n",
    "\n",
    "for i in range(1):\n",
    "    train_aug = prepare(train_ds_for_aug, shuffle = True, augment = True)\n",
    "    train_aug = train_aug.unbatch()\n",
    "    train_aug = train_aug.map(lambda x, y: (tf.cast(x, tf.float64), y))\n",
    "    train_ds = train_ds.concatenate(train_aug)\n",
    "\n",
    "del train_aug, train_ds_for_aug\n",
    "images = np.asarray(list(train_ds.map(lambda x, y: x)))\n",
    "labels = np.asarray(list(train_ds.map(lambda x, y: y)))\n",
    "clear_output(wait=False)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac92115b-2dcf-46b4-8914-8648367fc959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load pre-trained ResNet50 model and adjust for current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "badf7bb5-9268-416e-8a4c-fb06d4fbcbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "\n",
    "# Modify the ResNet50 model to accept grayscale images\n",
    "res_model = applications.ResNet50(include_top=False, weights=None, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze all layers except the last block\n",
    "for layer in res_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layers to verify\n",
    "for i, layer in enumerate(res_model.layers):\n",
    "    print(f\"Layer {i}: {layer.name}, Trainable: {layer.trainable}\")\n",
    "\n",
    "# Add a Conv2D layer to convert grayscale images to 3 channels\n",
    "input_layer = layers.Input(shape=(128, 128, 1))\n",
    "x = layers.Conv2D(3, (3, 3), padding='same')(input_layer)\n",
    "x = res_model(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4, activation='softmax')(x)  # Add Dense layer with number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa11a6e-81e5-4b4e-9315-5d7c4ff75e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "std_learning_rate = 1e-4\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate = std_learning_rate),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffcf68a-69f9-4753-8013-e2af4df5de47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "train_labels_cat = to_categorical(train_lab_idx.astype('int8'), num_classes=4)\n",
    "test_labels_cat = to_categorical(test_lab_idx.astype('int8'), num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data, train_labels_cat, epochs=10, \n",
    "    validation_data=(test_data, test_labels_cat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3202b367-b8e9-44f4-bfcb-d956542e157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10691e6c-753b-4bad-acef-f967f9b8581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax1.set_xlabel('Epochs', fontsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=18)\n",
    "ax1.plot(history.history['loss'], color='green', label='Training Loss', linewidth=2.5)\n",
    "ax1.plot(history.history['val_loss'], color='orange', linestyle='--', label='Validation Loss', linewidth=2.5)\n",
    "ax1.legend(loc='upper right', fontsize=18)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "ax2.set_xlabel('Epochs', fontsize=18)\n",
    "ax2.set_ylabel('Accuracy', fontsize=18)\n",
    "ax2.plot(history.history['accuracy'], color='green', label='Training Accuracy', linewidth=2.5)\n",
    "ax2.plot(history.history['val_accuracy'], color='orange', linestyle='--', label='Validation Accuracy', linewidth=2.5)\n",
    "ax2.legend(loc='upper left', fontsize=18)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bd0956-d734-487c-87c1-768b87c5357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fine tune the ResNet50\n",
    "\n",
    "Since this is a relatively small dataset and since we have limited computational resources, we only unfreeze the final few layers of the network (rather than all) for retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977dc1ff-a114-409e-bac0-6509f071b582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How many layers are in the base model\n",
    "print(\"Number of layers in ResNet50: \", len(res_model.layers))\n",
    "\n",
    "# For now, unfreeze last whole convolutional block for fine tuning\n",
    "fine_tune_at = 143\n",
    "for layer in res_model.layers[fine_tune_at:]:\n",
    "  layer.trainable = True\n",
    "\n",
    "# Print layers to verify\n",
    "for i, layer in enumerate(res_model.layers):\n",
    "    print(f\"Layer {i}: {layer.name}, Trainable: {layer.trainable}\")\n",
    "\n",
    "# Add a Conv2D layer to convert grayscale images to 3 channels\n",
    "input_layer = layers.Input(shape=(128, 128, 1))\n",
    "x = layers.Conv2D(3, (3, 3), padding='same')(input_layer)\n",
    "x = res_model(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4, activation='softmax')(x)  # Add Dense layer with number of classes\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "std_learning_rate = 1e-5 # use lower learning rate with more trainable layers\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate = std_learning_rate),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c01f103-2240-447e-a9b0-92022687696c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "train_labels_cat = to_categorical(train_lab_idx.astype('int8'), num_classes=4)\n",
    "test_labels_cat = to_categorical(test_lab_idx.astype('int8'), num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data, train_labels_cat, epochs=10, \n",
    "    validation_data=(test_data, test_labels_cat)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae65e3c-589c-42e3-a5a1-001d2ac089ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax1.set_xlabel('Epochs', fontsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=18)\n",
    "ax1.plot(history.history['loss'], color='blue', label='Training Loss', linewidth=2.5)\n",
    "ax1.plot(history.history['val_loss'], color='red', linestyle='--', label='Validation Loss', linewidth=2.5)\n",
    "ax1.legend(loc='upper right', fontsize=18)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "ax2.set_xlabel('Epochs', fontsize=18)\n",
    "ax2.set_ylabel('Accuracy', fontsize=18)\n",
    "ax2.plot(history.history['accuracy'], color='blue', label='Training Accuracy', linewidth=2.5)\n",
    "ax2.plot(history.history['val_accuracy'], color='red', linestyle='--', label='Validation Accuracy', linewidth=2.5)\n",
    "ax2.legend(loc='upper left', fontsize=18)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ResNet50_transfer_learning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
