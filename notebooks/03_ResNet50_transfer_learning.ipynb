{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee3eca32-5942-4c10-a19f-84e17aad6c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook explores the use of transfer learning with ResNet50 to classify Alzheimer's disease from anatomical MRI images.\n",
    "\n",
    "Briefly, the pipeline involves the following steps and technical features:\n",
    "\n",
    "- Data formating and quality check\n",
    "- Transfer learning using ResNet50\n",
    "- Hyperparameter tuning\n",
    "- Final model application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "849d4204-28eb-4075-9b98-d440d01601c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import analysis and plotting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018f7e13-4633-4a51-a4d1-25cdd08d7c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# extract aws credentials from hidden table \n",
    "aws_keys_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \",\").load(\"/FileStore/tables/brad_databricks_personal_accessKeys_new.csv\")\n",
    "\n",
    "ACCESS_KEY = aws_keys_df.collect()[0][0]\n",
    "SECRET_KEY = aws_keys_df.collect()[0][1]\n",
    "\n",
    "# specify bucket and mount point\n",
    "AWS_S3_BUCKET = \"databricks-workspace-stack-brad-personal-bucket/AD_MRI_classification/\"\n",
    "MOUNT_NAME = \"/mnt/AD_classification\"\n",
    "SOURCE_URL = f\"s3a://{AWS_S3_BUCKET}\"\n",
    "EXTRA_CONFIGS = { \"fs.s3a.access.key\": ACCESS_KEY, \"fs.s3a.secret.key\": SECRET_KEY}\n",
    "\n",
    "# mount bucket\n",
    "# dbutils.fs.unmount(MOUNT_NAME)\n",
    "# dbutils.fs.mount(SOURCE_URL, MOUNT_NAME, extra_configs = EXTRA_CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ade3180-eaee-420f-b1b2-829033f85097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning and statistics\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras import applications, layers, models, applications, callbacks, optimizers\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.stats import false_discovery_control\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel computing\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import cv2\n",
    "import magic\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f8c2c9-9522-4139-b9a6-02d8c1e06d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and adapt ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "badf7bb5-9268-416e-8a4c-fb06d4fbcbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "\n",
    "# Modify the ResNet50 model to accept grayscale images\n",
    "res_model = applications.ResNet50(include_top=False, weights=None, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze all layers except the last block\n",
    "for layer in res_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layers to verify\n",
    "for i, layer in enumerate(res_model.layers):\n",
    "    print(f\"Layer {i}: {layer.name}, Trainable: {layer.trainable}\")\n",
    "\n",
    "# Add a Conv2D layer to convert grayscale images to 3 channels\n",
    "input_layer = layers.Input(shape=(128, 128, 1))\n",
    "x = layers.Conv2D(3, (3, 3), padding='same')(input_layer)\n",
    "x = res_model(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4, activation='softmax')(x)  # Add Dense layer with number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa11a6e-81e5-4b4e-9315-5d7c4ff75e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "std_learning_rate = 1e-4\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate = std_learning_rate),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffcf68a-69f9-4753-8013-e2af4df5de47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "train_labels_cat = to_categorical(train_lab_idx.astype('int8'), num_classes=4)\n",
    "test_labels_cat = to_categorical(test_lab_idx.astype('int8'), num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data, train_labels_cat, epochs=10, \n",
    "    validation_data=(test_data, test_labels_cat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3202b367-b8e9-44f4-bfcb-d956542e157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10691e6c-753b-4bad-acef-f967f9b8581f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax1.set_xlabel('Epochs', fontsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=18)\n",
    "ax1.plot(history.history['loss'], color='green', label='Training Loss', linewidth=2.5)\n",
    "ax1.plot(history.history['val_loss'], color='orange', linestyle='--', label='Validation Loss', linewidth=2.5)\n",
    "ax1.legend(loc='upper right', fontsize=18)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "ax2.set_xlabel('Epochs', fontsize=18)\n",
    "ax2.set_ylabel('Accuracy', fontsize=18)\n",
    "ax2.plot(history.history['accuracy'], color='green', label='Training Accuracy', linewidth=2.5)\n",
    "ax2.plot(history.history['val_accuracy'], color='orange', linestyle='--', label='Validation Accuracy', linewidth=2.5)\n",
    "ax2.legend(loc='upper left', fontsize=18)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bd0956-d734-487c-87c1-768b87c5357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fine tune the ResNet50\n",
    "\n",
    "Since this is a relatively small dataset and since we have limited computational resources, we only unfreeze the final few layers of the network (rather than all) for retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977dc1ff-a114-409e-bac0-6509f071b582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How many layers are in the base model\n",
    "print(\"Number of layers in ResNet50: \", len(res_model.layers))\n",
    "\n",
    "# For now, unfreeze last whole convolutional block for fine tuning\n",
    "fine_tune_at = 143\n",
    "for layer in res_model.layers[fine_tune_at:]:\n",
    "  layer.trainable = True\n",
    "\n",
    "# Print layers to verify\n",
    "for i, layer in enumerate(res_model.layers):\n",
    "    print(f\"Layer {i}: {layer.name}, Trainable: {layer.trainable}\")\n",
    "\n",
    "# Add a Conv2D layer to convert grayscale images to 3 channels\n",
    "input_layer = layers.Input(shape=(128, 128, 1))\n",
    "x = layers.Conv2D(3, (3, 3), padding='same')(input_layer)\n",
    "x = res_model(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4, activation='softmax')(x)  # Add Dense layer with number of classes\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "std_learning_rate = 1e-5 # use lower learning rate with more trainable layers\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate = std_learning_rate),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c01f103-2240-447e-a9b0-92022687696c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "train_labels_cat = to_categorical(train_lab_idx.astype('int8'), num_classes=4)\n",
    "test_labels_cat = to_categorical(test_lab_idx.astype('int8'), num_classes=4)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data, train_labels_cat, epochs=10, \n",
    "    validation_data=(test_data, test_labels_cat)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae65e3c-589c-42e3-a5a1-001d2ac089ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax1.set_xlabel('Epochs', fontsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=18)\n",
    "ax1.plot(history.history['loss'], color='blue', label='Training Loss', linewidth=2.5)\n",
    "ax1.plot(history.history['val_loss'], color='red', linestyle='--', label='Validation Loss', linewidth=2.5)\n",
    "ax1.legend(loc='upper right', fontsize=18)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "ax2.set_xlabel('Epochs', fontsize=18)\n",
    "ax2.set_ylabel('Accuracy', fontsize=18)\n",
    "ax2.plot(history.history['accuracy'], color='blue', label='Training Accuracy', linewidth=2.5)\n",
    "ax2.plot(history.history['val_accuracy'], color='red', linestyle='--', label='Validation Accuracy', linewidth=2.5)\n",
    "ax2.legend(loc='upper left', fontsize=18)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ResNet50_transfer_learning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
