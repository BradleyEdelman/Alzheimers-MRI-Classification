{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885d8509-03bf-42c0-8693-3025249593d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Alzheimer's Disease classification from anatomical MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cf98b28-d111-4bd3-ae7a-4cbb1db88ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This notebook explores the use of a low-dimensional feature space to classify Alzheimer's disease from anatomical MRI images.\n",
    " \n",
    "Briefly, the pipeline involves the following steps and technical features:\n",
    "\n",
    "- Data formating and quality check\n",
    "- Feature extraction and visualization\n",
    "- Synthetic class balancing\n",
    "- Random forest classification as a function of features (principal components) used\n",
    "- Permutation testing for statistical significance (parallel processing)\n",
    "- Feature interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e64237c-6420-4fee-9bb0-7591bd6ed724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import analysis and plotting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62588c7-7321-4b0c-9628-a47a287ae8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning and statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from scipy.stats import false_discovery_control\n",
    "\n",
    "# Parallel computing\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import cv2\n",
    "import magic\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0134ba-e908-44e4-bc28-a90652b7f351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and format training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dff515e-e2d0-4b37-a5dd-adac60f3852b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"C:/Users/bedelman/Documents\\GitHub/Alzheimers-MRI-Classification/Alzheimer_MRI_Dataset/Data/\"\n",
    "\n",
    "'''\n",
    "Label meanings\n",
    "0 - Mild dementia\n",
    "1 - Moderate dementia\n",
    "2 - No dementia\n",
    "3 - Very mild dementia\n",
    "'''\n",
    "Lab = ['Mild', 'Moderate', 'None', 'Very Mild']\n",
    "\n",
    "train = pd.read_parquet(f\"{BASE_DIR}/train-00000-of-00001-c08a401c53fe5312.parquet\", engine = \"pyarrow\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0552ebf8-3d6b-410a-92c6-a8445de46900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Convert data to readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8baa17e8-7c54-4c32-ae59-0fa5d78b5772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dict_to_image(image_dict):\n",
    "    if isinstance(image_dict, dict) and 'bytes' in image_dict:\n",
    "        byte_string = image_dict['bytes']\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
    "        return img\n",
    "    else:\n",
    "        raise TypeError(f\"Expected dictionary with 'bytes' key, got {type(image_dict)}\")\n",
    "\n",
    "train['img_arr'] = train['image'].apply(dict_to_image)\n",
    "train.drop(\"image\", axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6f17f6-bc1c-4800-8abc-cca542440a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and format test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f6d8f0-f1a2-4dd4-b9f0-a2dbd4f76725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f\"{BASE_DIR}/test-00000-of-00001-44110b9df98c5585.parquet\", engine = \"pyarrow\")\n",
    "test.head() \n",
    "\n",
    "# Also convert to readable format\n",
    "test['img_arr'] = test['image'].apply(dict_to_image)\n",
    "test.drop(\"image\", axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30639138-6cbf-41f3-87d0-b7bb9924c3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore structure and visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70e2567b-e701-4d31-9069-97990e88fd00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visually inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ee8d26-6bb2-42b1-a7da-5ada66fc3533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lab_idx = np.asarray(train.iloc[:].label)\n",
    "\n",
    "f, ax = plt.subplots(4,4)\n",
    "for lab in range(4):\n",
    "    for ex in range(4):\n",
    "    \n",
    "        class_lab = np.argwhere(train_lab_idx == 1)\n",
    "        current_idx = np.random.randint(len(class_lab)-1,size = 1)\n",
    "        current_idx = np.asarray(current_idx)\n",
    "        \n",
    "        ax[ex, lab].axis('off')\n",
    "        ax[ex, lab].imshow(train.iloc[class_lab[current_idx[0]][0]].img_arr, cmap = \"gray\")\n",
    "        if ex == 0: ax[ex, lab].set_title(Lab[lab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae3ba54c-fc8a-4f61-8f28-d5ca74ad2c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distribution of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39000e36-c2f6-4a1b-b84e-effd18a9ce96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,3)\n",
    "unique, counts = np.unique(np.asarray(train.iloc[:].label), return_counts = True)\n",
    "ax[0].bar(unique, counts)\n",
    "ax[0].set_xticks(unique)\n",
    "ax[0].set_xticklabels(Lab, rotation = 45)\n",
    "ax[0].set_title('Training')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('# of images')\n",
    "\n",
    "ax[1].axis('off')\n",
    "\n",
    "unique, counts = np.unique(np.asarray(test.iloc[:].label), return_counts = True)\n",
    "ax[2].bar(unique, counts)\n",
    "ax[2].set_xticks(unique)\n",
    "ax[2].set_xticklabels(Lab, rotation = 45)\n",
    "ax[2].set_title('Testing')\n",
    "ax[2].set_xlabel('Class')\n",
    "ax[0].set_ylabel('# of images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1965b20a-1e24-471c-bd49-02049ea94291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### We can clearly see that there is an imbalance across the different stages of AD. Since the smallest class contains very few images, undersampling the other three classes would provide very little data for fitting a model. Instead, we will syntheticall increase the number of images in the three smaller classes to match the number of images in the largest class. To do this, while also avoiding overfitting, we will implement the Synthetic Minority Over-sampling TEchnique (SMOTE) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6a9c46-2bc8-49b9-95a5-d9227a6729e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def SMOTE(sample: np.array, N: int, k: int) -> np.array:\n",
    "    \n",
    "    T, num_attrs = sample.shape\n",
    "    \n",
    "    # If N is less than 100%, randomize the minority class samples as only a random percent of them will be SMOTEd\n",
    "    if N < 100:\n",
    "        T = round(N / 100 * T)\n",
    "        N = 100\n",
    "    # The amount of SMOTE is assumed to be in integral multiples of 100\n",
    "    N = int(N / 100)\n",
    "    synthetic = np.zeros([T * N, num_attrs])\n",
    "    new_index = 0\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(sample.values)\n",
    "    \n",
    "def populate(N, i, nnarray):\n",
    "        \n",
    "    nonlocal new_index\n",
    "    nonlocal synthetic\n",
    "    nonlocal sample\n",
    "    while N != 0:\n",
    "            nn = randrange(1, k+1)\n",
    "            for attr in range(num_attrs):\n",
    "                dif = sample.iloc[nnarray[nn]][attr] - sample.iloc[i][attr]\n",
    "                gap = uniform(0, 1)\n",
    "                synthetic[new_index][attr] = sample.iloc[i][attr] + gap * dif\n",
    "            new_index += 1\n",
    "            N = N - 1\n",
    "    \n",
    "    for i in range(T):\n",
    "        nnarray = nbrs.kneighbors(sample.iloc[i].values.reshape(1, -1), return_distance=False)[0]\n",
    "        populate(N, i, nnarray)\n",
    "    \n",
    "    return synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32692fb4-fadb-4e8e-84b2-d63ded1662ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee893766-8c77-4329-9d87-e3412477ddcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utilize simple PCA decomposition\n",
    "\n",
    "- Can exmine classification as a function of data variance\n",
    "- Provides spatial maps to help with interpretation of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051efcaf-16a6-4457-9ce8-af2eac9fe412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(train)):\n",
    "    train_data.append(np.reshape(train.iloc[i].img_arr, (1, -1)))    \n",
    "train_data = np.vstack(train_data[:])\n",
    "\n",
    "# decompose with PCA and look at various metrics/info\n",
    "pca = PCA(n_components = 100)\n",
    "pca.fit(train_data)\n",
    "\n",
    "plt.plot(np.linspace(1,100,100),pca.explained_variance_[:100]/sum(pca.explained_variance_[:100])*100,'b')\n",
    "plt.title('PCA')\n",
    "plt.xlabel('Component #')\n",
    "plt.ylabel('Variance Explained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e632072-c1c9-4c7b-9321-1e5e21f693f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Project training data into PCA space to visualize potential clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d63e6b5-c9da-4bec-8048-5142e87b6cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### First, use first two PC's for simple visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c6afaa-0e3a-4581-a8cc-d5820602537a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "load_train = np.matmul(train_data, loadings[:,:2])\n",
    "\n",
    "# plot first two PCs\n",
    "s  = plt.scatter(load_train[:,0], load_train[:,1], c = train_lab_idx*2, cmap = 'tab10', alpha = 0.75)\n",
    "handles, labels = s.legend_elements()\n",
    "legend = plt.legend(handles = handles, labels = Lab, title = 'Diagnosis', loc = 'upper right')\n",
    "plt.axis('off')\n",
    "plt.title('PCA projections')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c59f92-08e0-4513-a2a4-4d555fff5d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply random forest classifier to first two PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c7c4cc-e618-4b93-bf05-fd0221d86378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train random forest classifier and apply to test data\n",
    "## for now again only with first two PCs\n",
    "\n",
    "rf_1 = RandomForestClassifier()\n",
    "\n",
    "# Train classifier\n",
    "load_train = np.matmul(train_data, loadings[:,:2])\n",
    "rf_1.fit(load_train, train_lab_idx)\n",
    "\n",
    "# Properly format test data and extract same features (PC loadings)\n",
    "test_data = []\n",
    "for i in range(len(test)):\n",
    "    test_data.append(np.reshape(test.iloc[i].img_arr, (1, -1)))    \n",
    "test_data = np.vstack(test_data[:])\n",
    "\n",
    "load_test = np.matmul(test_data, loadings[:,:2])\n",
    "\n",
    "# Predict unseen data\n",
    "predictions_1 = rf_1.predict(load_test)\n",
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "\n",
    "# compare predictions with test labels and compute accuracy\n",
    "result = predictions_1 - test_lab_idx\n",
    "result_binary = np.argwhere(result == 0)\n",
    "correct = np.size(result_binary,0)\n",
    "acc = correct/test_lab_idx.shape[0]*100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "124699c7-5bb9-4417-a366-22b1b75de70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### When using the top two PCs, we see a modest classification rate on the test set. However, using two PC's here is arbitrary and is easy simply due to visualization purposes. We can also exmaine accuracy as a function of the number of PCs and visualize the PC loading maps to interpret the classification results. But first, let's run some statistics to determine if the classification is significant in its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0231ff-6e49-44a1-80fe-ecdb19f148e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Permutation testing on shuffled data as a control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e627c742-c323-4e62-a12a-0a7999f5188c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parallelize time-consuming iterative processes with Dask (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3edf653-9c21-4f55-afb4-4c923fbacbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker = 4, n_workers = 1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c33448e8-f3ad-4c90-998b-b5c153fa3397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shuffle data labels in the training set when building RF classifier\n",
    "\n",
    "#### Do this a number of times to generate a null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997a48ef-a218-4eae-84ca-3ac95a3efac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "train_lab_idx_shuffle = train_lab_idx\n",
    "load_train = np.matmul(train_data, loadings[:,:2])\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def classification_RF_shuffle(load_train, train_lab_idx_shuffle, load_test, test_lab_idx):\n",
    "    \n",
    "    # Train classifier with new shuffled labels each iteration\n",
    "    rf_1 = RandomForestClassifier()\n",
    "    rng.shuffle(train_lab_idx_shuffle)\n",
    "    rf_1.fit(load_train, train_lab_idx_shuffle)\n",
    "\n",
    "    # Predict unseen data\n",
    "    predictions_1 = rf_1.predict(load_test)\n",
    "    test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "\n",
    "    # compare predictions with test labels and compute accuracy\n",
    "    result = predictions_1 - test_lab_idx\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary,0)\n",
    "    acc = correct/test_lab_idx.shape[0]*100\n",
    "    return acc\n",
    "\n",
    "# Parallelize lazy function to reduce computation time\n",
    "## limit number of iterations in this function to account for computer memory limit\n",
    "def iteration_permute(load_train, train_lab_idx_shuffle, load_test, test_lab_idx):\n",
    "    \n",
    "    acc_shuffle = []\n",
    "    for i in range(100):\n",
    "        acc_tmp = dask.delayed(classification_RF_shuffle)(load_train, train_lab_idx_shuffle, load_test, test_lab_idx)\n",
    "        acc_shuffle.append(acc_tmp)\n",
    "    \n",
    "    # compute values\n",
    "    acc_shuffle = dask.compute(*acc_shuffle)\n",
    "    acc_shuffle = np.array([float(num) for num in acc_shuffle])\n",
    "    clear_output(wait = True)\n",
    "    return acc_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40c9ebed-01df-4265-8b92-f8653547fc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Permute a sufficient number of times (here 500)\n",
    "\n",
    "### Not the most efficient way to do this, but must consider memory on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff9ea68-a750-4900-9c4d-e7585c81568b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc_shuffle_compile = []\n",
    "for i in range(5):\n",
    "    acc_tmp = iteration_permute(load_train, train_lab_idx_shuffle, load_test, test_lab_idx)\n",
    "    acc_shuffle_compile = np.concatenate((acc_shuffle_compile, acc_tmp), axis=0)\n",
    "\n",
    "b = plt.boxplot(acc_shuffle_compile)\n",
    "plt.title('Shuffled Classification Accuracy')\n",
    "plt.xlabel('Shuffled')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9dc29da-d673-4fb8-acd8-aa2af0a55009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Side note: validate parallel processing by comparing computation against serial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82399334-dca9-47cb-9154-fc90aab623f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# simple test to ensure that parallel processing is faster\n",
    "\n",
    "# traditional for loop\n",
    "acc_shuffle_nodask = []\n",
    "for i in range(100):\n",
    "    acc_tmp = classification_RF_shuffle(load_train, train_lab_idx, load_test, test_lab_idx)\n",
    "    acc_shuffle_nodask.append(acc_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a7aac85-4d87-4ea3-a06a-0f40e2a0370d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# dask parallelization\n",
    "acc_shuffle_dask = []\n",
    "for i in range(1):\n",
    "    acc_tmp = iteration_permute(load_train, train_lab_idx_shuffle, load_test, test_lab_idx)\n",
    "    acc_shuffle_dask = np.concatenate((acc_shuffle_dask, acc_tmp), axis=0)\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61296cc5-0688-476b-bde8-cbafd6a0ca0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### We can in fact see that parallelization with dask does markedly reduce the computation time (on the CPU). Of course, IO data transfer is the bottleneck here, but this will reduce computation time enough for this example on a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed65ada2-3434-4240-9ebf-15e5459eb746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Determine significance of classification accuracy (permutation test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b286943-365b-4b97-9af6-ce36969a8127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_above_value = (acc_shuffle_compile > acc).sum()\n",
    "p_value = null_above_value/np.size(acc_shuffle_compile,0)\n",
    "print(p_value)\n",
    "\n",
    "# We can see here that the classification accuracy is not sigificcantly different from chance with p-value > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2141aa07-eac8-40c2-8133-d682e62ce9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Now explore RF classification as a function of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c789c247-0ed4-4d9e-be09-5860ea80ee95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def classification_RF_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, num_pc):\n",
    "\n",
    "    load_train = np.matmul(train_data, loadings[:,:num_pc])\n",
    "    \n",
    "    rf_1 = RandomForestClassifier()\n",
    "    rf_1.fit(load_train, train_lab_idx)\n",
    "    \n",
    "    load_test = np.matmul(test_data, loadings[:,:num_pc])\n",
    "    \n",
    "    # Predict unseen data\n",
    "    predictions_1 = rf_1.predict(load_test)\n",
    "    test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "    \n",
    "    # compare predictions with test labels and compute accuracy\n",
    "    result = predictions_1 - test_lab_idx\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary,0)\n",
    "    acc = correct/test_lab_idx.shape[0]*100\n",
    "    return acc\n",
    "\n",
    "def iteration_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, pc_idx):\n",
    "    \n",
    "    acc_PC = []\n",
    "    for i in pc_idx:\n",
    "        acc_tmp = dask.delayed(classification_RF_PCs)(train_data, train_lab_idx, test_data, test_lab_idx, loadings, i)\n",
    "        acc_PC.append(acc_tmp)\n",
    "    \n",
    "    # compute values\n",
    "    acc_PC = dask.compute(*acc_PC)\n",
    "    clear_output(wait = True)\n",
    "    acc_PC = np.array([float(num) for num in acc_PC])\n",
    "    return acc_PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262282e1-a9a7-494b-81c4-5283c1e762cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch parallel processing to conserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e82591-0f51-4c47-863f-9827730efe70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = [np.linspace(1,50,50, dtype = 'int'), np.linspace(51,100,50, dtype = 'int')]\n",
    "\n",
    "acc_PC_compile = []\n",
    "for i in range(len(idx)):\n",
    "    \n",
    "    acc_tmp = iteration_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, idx[i])\n",
    "    acc_PC_compile = np.concatenate((acc_PC_compile, acc_tmp), axis = 0)\n",
    "\n",
    "# plot accuracy as a function of PCs\n",
    "plt.plot(np.linspace(1,np.size(acc_PC_compile,0), np.size(acc_PC_compile,0)), acc_PC_compile, 'k')\n",
    "plt.title('Classification accuracy')\n",
    "plt.xlabel('# of PCs included')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 100)\n",
    "plt.xlim(-5, 105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad4e16b-cb8d-48f8-9478-6d5249e8591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Permutation testing for each number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df58d3a8-c84c-4118-9cc9-5265a27cfc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def classification_RF_shuffle_PCs(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, num_pc):\n",
    "\n",
    "    load_train = np.matmul(train_data, loadings[:,:num_pc])\n",
    "    \n",
    "    rf_1 = RandomForestClassifier()\n",
    "    rng.shuffle(train_lab_idx_shuffle)\n",
    "    rf_1.fit(load_train, train_lab_idx_shuffle)\n",
    "    \n",
    "    load_test = np.matmul(test_data, loadings[:,:num_pc])\n",
    "    \n",
    "    # Predict unseen data\n",
    "    predictions_1 = rf_1.predict(load_test)\n",
    "    test_lab_idx = np.asarray(test.iloc[:].label)\n",
    "    \n",
    "    # compare predictions with test labels and compute accuracy\n",
    "    result = predictions_1 - test_lab_idx\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary,0)\n",
    "    acc = correct/test_lab_idx.shape[0]*100\n",
    "    return acc\n",
    "\n",
    "def iteration_permute_PCs(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, num_pc):\n",
    "    \n",
    "    acc_shuffle = []\n",
    "    for i in range(100):\n",
    "        acc_tmp = dask.delayed(classification_RF_shuffle_PCs)(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, num_pc)\n",
    "        acc_shuffle.append(acc_tmp)\n",
    "    \n",
    "    # compute values\n",
    "    acc_shuffle = dask.compute(*acc_shuffle)\n",
    "    acc_shuffle = np.array([float(num) for num in acc_shuffle])\n",
    "    clear_output(wait = True)\n",
    "    return acc_shuffle\n",
    "\n",
    "# def iteration_shuffle_PCs(train_data, train_lab_idx, test_data, test_lab_idx, loadings, pc_idx):\n",
    "    \n",
    "#     acc_PC = []\n",
    "#     for i in pc_idx:\n",
    "#         acc_tmp = dask.delayed(iteration_permute_PCs)(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, i)\n",
    "#         acc_PC.append(acc_tmp)\n",
    "    \n",
    "#     # compute values\n",
    "#     acc_PC = dask.compute(*acc_PC)\n",
    "#     clear_output(wait = True)\n",
    "#     # acc_PC = np.array([float(num) for num in acc_PC])\n",
    "#     return acc_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f274d29-3cec-4973-9e72-14e5cd9fca02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = np.linspace(1,100,100, dtype = 'int')\n",
    "acc_shuffle_PC_compile = []\n",
    "for i in idx:\n",
    "    acc_tmp = iteration_permute_PCs(train_data, train_lab_idx_shuffle, test_data, test_lab_idx, loadings, i)\n",
    "    acc_shuffle_PC_compile.append(acc_tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd8e7e3-1952-4f19-ab0c-2b400ada08ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Compare classification accuracy to chance level for each number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "573cd8bf-998b-44d1-8e34-ef5e59a0c44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot accuracy as a function of PCs\n",
    "\n",
    "# accuracy as a function of PCs\n",
    "plt.plot(np.linspace(1,np.size(acc_PC_compile,0), np.size(acc_PC_compile,0)), acc_PC_compile, 'k')\n",
    "\n",
    "# corresponding chance level\n",
    "p_value_compile = []\n",
    "for i in range(len(acc_shuffle_PC_compile)):\n",
    "    b = plt.boxplot(acc_shuffle_PC_compile[i], 'b', positions = [i+1], flierprops = {'marker': '.', 'markersize': 5})\n",
    "\n",
    "    # permutation testing\n",
    "    null_above_value = (acc_shuffle_PC_compile[i] > acc_PC_compile[i]).sum()\n",
    "    p_value_compile.append(null_above_value/np.size(acc_shuffle_PC_compile[i],0))\n",
    "\n",
    "# visualize statistical results\n",
    "p_value_compile = np.array([float(num) for num in p_value_compile])\n",
    "p_value_adj = false_discovery_control(p_value_compile)   \n",
    "\n",
    "for i in range(len(p_value_adj)): \n",
    "    if p_value_adj[i] < 0.05:\n",
    "        plt.scatter(i, 95, s = 10, c = 'r', marker = \"*\")\n",
    "\n",
    "plt.title('Classification accuracy')\n",
    "plt.xlabel('# of PCs included')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 100)\n",
    "plt.xlim(-5, 105)\n",
    "p_value_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b1a48c-0b1a-4ca9-add8-38b42a158d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature interpretation\n",
    "\n",
    "### Visualize spatial PC maps to identify features that contribute to data variance and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd55ec8c-70a6-466c-9843-f56fc99e2806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(5,4)\n",
    "lim = 20\n",
    "for ipc in range(lim):\n",
    "\n",
    "    PC_map = loadings[:,ipc]\n",
    "    PC_map = np.reshape(PC_map, (128,128))\n",
    "    clim = max(abs(np.min(PC_map)), abs(np.max(PC_map)))\n",
    "\n",
    "    idx1 = np.floor(ipc/4).astype(int)\n",
    "    idx2 = np.fmod(ipc,4)\n",
    "    ax[idx1, idx2].axis('off')\n",
    "    ax[idx1, idx2].imshow(PC_map, cmap = \"magma\", vmin = -clim, vmax = clim)\n",
    "    ax[idx1, idx2].set_title(f'PC# {ipc +1}', fontsize = 10)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f923ef6c-0aab-44d9-9bf9-58fc7705d88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Here we can see that for each of the top 20 PC's there are brain regions are more strongly weighted than others (in either the positive or negative direction). It is important to note that it does not appear that each image used in this dataset came from the same location/slice within the brain. We do not know if certain slices are over- or under-represented in different AD classes and therefore cannot say with certainty whether specific anatomical structures drive different stages of AD. Nevertheless,"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Alzheimers MRI classification - RandomForest-checkpoint",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
