{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf98b28-d111-4bd3-ae7a-4cbb1db88ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Implements a random forest classifier to classify Alzheimer's disease, with steps including:\n",
    "- Applying Principal Component Analysis (PCA) for dimensionality reduction and feature extraction\n",
    "- Cross-validation of classification results\n",
    "- Hyperparameter tuning\n",
    "- Permutation testing for statistical significance using parallel processing\n",
    "- Exploring feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62588c7-7321-4b0c-9628-a47a287ae8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"standard\"\n",
    "import numpy as np\n",
    "\n",
    "# machine learning and statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import false_discovery_control, randint\n",
    "import dask.array as da\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# misc\n",
    "import time, os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys, pickle\n",
    "from pathlib import Path\n",
    "script_dir = Path(os.getcwd()).parent\n",
    "src_dir = script_dir / 'src'\n",
    "sys.path.append(str(src_dir))\n",
    "from random_forest import RF, classification_RF_shuffle, iteration_permute, iteration_PCs, iteration_permute_PCs\n",
    "from visualize import multiclass_summary\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = Path(os.getcwd()).parent\n",
    "results_dir_00 = script_dir / 'results' / 'notebook_00'\n",
    "\n",
    "results_dir_01 = script_dir / 'results' / 'notebook_01'\n",
    "os.makedirs(results_dir_01, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7521c2-f1a3-45a7-aa3d-56e69d2b32e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data file and unpack contents\n",
    "file_name = results_dir_00 / 'data_preprocessed_xfold.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    data_preproc_xfold = pickle.load(f)\n",
    "\n",
    "data=data_preproc_xfold['data']\n",
    "labels=np.array(data_preproc_xfold['labels'])\n",
    "\n",
    "train_idx_fold=data_preproc_xfold['train_idx_fold']\n",
    "test_idx_fold=data_preproc_xfold['test_idx_fold']\n",
    "\n",
    "class_lab = ['Mild', 'Moderate', 'None', 'Very Mild']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e816ac11-a034-4abe-aa20-ac36a22a0584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the current case, we can think of each pixel representing a single feature. However, since most ensemble learning approaches, including random forest, are not equiped to deal with hundreds of thousands of features, we need to reduce the dimensionality of the feature space. This can be done with feature selection approaches, however, in this case such features would be individual pixels and would be prone to noise and alignment errors. In contrast, principal component analysis (PCA) transforms the data into a new set of uncorrelated variables (principal components) that retain as much variance as possible. This allows feature space to be compressed while preserving the most informative aspects of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763a32fb-6a7b-48e0-9892-407100f1657c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_pca = []\n",
    "for i in range(len(data)):\n",
    "    data_pca.append(np.reshape(data[i], (1, -1)))    \n",
    "data_pca = np.vstack(data_pca[:])\n",
    "\n",
    "# decompose with PCA and look at various metrics/info\n",
    "num_comp = 50\n",
    "pca = PCA(n_components = num_comp)\n",
    "pca.fit(data_pca)\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8d16da-77eb-4d63-8f09-e4282a7db410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Visualize variance explained and 2D PC representation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a3c60a-4061-4e8e-a226-491e3aaf48f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# project data on first two PC's\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "load_tot = np.matmul(data_pca, loadings[:,:2])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot variance explained\n",
    "ax1.plot(np.linspace(1, num_comp, num_comp), pca.explained_variance_[:num_comp]/sum(pca.explained_variance_[:num_comp]) * 100, 'b')\n",
    "ax1.set_title('PCA')\n",
    "ax1.set_xlabel('Component #')\n",
    "ax1.set_ylabel('Variance Explained')\n",
    "\n",
    "# Plot first two PCs\n",
    "s = ax2.scatter(load_tot[:, 0], load_tot[:, 1], c=labels * 2, cmap='tab10', alpha=0.75)\n",
    "handles, lab = s.legend_elements()\n",
    "legend = ax2.legend(handles=handles, labels=class_lab, title='Diagnosis', loc='upper right')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('PCA projections')\n",
    "ax2.set_xlabel('PC 1')\n",
    "ax2.set_ylabel('PC 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c59f92-08e0-4513-a2a4-4d555fff5d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ensemble learning methods, such as Random Forests, are well-suited for imbalanced classification problems due to their ability to aggregate predictions from multiple weak learners, which helps reduce variance and improve robustness. Additionally, ensemble models contain inherent averaging mechanisms that can reduce the bias introduced by imbalanced datasets. Here, we apply a Random Forest classifier to first two PCs with 5-fold stratified cross validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c7c4cc-e618-4b93-bf05-fd0221d86378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Perform X-fold cross validation\n",
    "data_load = np.matmul(data_pca, loadings[:,:2])\n",
    "\n",
    "predictions_fold = []\n",
    "true_labels_fold = []\n",
    "for i in range(len(train_idx_fold)):\n",
    "    train_data = data_load[train_idx_fold[i],:]\n",
    "    train_labels = labels[train_idx_fold[i]]\n",
    "    test_data = data_load[test_idx_fold[i],:]\n",
    "    test_labels = labels[test_idx_fold[i]]\n",
    "    predictions, true_labels = RF(train_data, train_labels, test_data, test_labels)\n",
    "    \n",
    "    predictions_fold.append(predictions)\n",
    "    true_labels_fold.append(true_labels)\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32d856e-32ee-4e8f-ad1e-c6a867aeab98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Summarize default results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57267c0-1f19-4876-9fc0-8c69ab51389d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare predictions with test labels and compute accuracy\n",
    "acc_fold = []\n",
    "for fold in range(len(predictions_fold)):\n",
    "    result = predictions_fold[fold] - true_labels_fold[fold]\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary, 0)\n",
    "    acc_fold.append(correct / true_labels_fold[fold].shape[0] * 100)\n",
    "    print(f\"Classification accuracy for fold {fold+1} = {acc_fold[fold]:.2f} %\")\n",
    "\n",
    "print()\n",
    "print(f\"Overall classification accuracy is: {np.mean(acc_fold):.2f} %\")\n",
    "print()\n",
    "\n",
    "# Visualize summary of predictions\n",
    "multiclass_summary(np.concatenate(predictions_fold, axis=0), np.concatenate(true_labels_fold, axis=0), class_lab)\n",
    "print()\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"predictions\": predictions_fold,\n",
    "    \"true_labels\": true_labels_fold,\n",
    "    \"accuracy\": acc_fold\n",
    "}\n",
    "\n",
    "file_name = results_dir_01 / 'RF_2PC_acc.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(f\"Data saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the top two PCs, we see a modest classification rate on the test set. However, there is a clear bias here towards the Moderate AD class, which had the largest bias in the training dataset. Here, we also used an off-the-shelf Random Forest with default parameters, which MAY explain the relatively low accuracy. We can optimize this approach by tuning various parameters and selecting the best ones for future testing, if needed. For this, we will tune parameters across folds to find the optimal parameters, and then apply these parameters to the original individual folds to evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': randint(2, 20),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "search = RandomizedSearchCV(rf_base, param_distributions=param_dist,\n",
    "                            n_iter=50, scoring='accuracy', cv=cv, random_state=42, n_jobs=-1)\n",
    "search.fit(data_load, labels)\n",
    "\n",
    "print(\"Best hyperparameters:\", search.best_params_)\n",
    "print()\n",
    "tuned_model = search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform X-fold cross validation with tuned model\n",
    "predictions_fold_tune = []\n",
    "true_labels_fold_tune = []\n",
    "\n",
    "for i in range(len(train_idx_fold)):\n",
    "    train_data = data_load[train_idx_fold[i]]\n",
    "    train_labels = labels[train_idx_fold[i]]\n",
    "    test_data = data_load[test_idx_fold[i]]\n",
    "    test_labels = labels[test_idx_fold[i]]\n",
    "\n",
    "    model = tuned_model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    predictions_fold_tune.append(predictions)\n",
    "    true_labels_fold_tune.append(test_labels)\n",
    "\n",
    "# Compare predictions with test labels and compute accuracy\n",
    "acc_fold_tune = []\n",
    "for fold in range(len(predictions_fold_tune)):\n",
    "    result = predictions_fold_tune[fold] - true_labels_fold_tune[fold]\n",
    "    result_binary = np.argwhere(result == 0)\n",
    "    correct = np.size(result_binary, 0)\n",
    "    acc_fold_tune.append(correct / true_labels_fold_tune[fold].shape[0] * 100)\n",
    "    print(f\"Tuned classification accuracy for fold {fold+1} = {acc_fold_tune[fold]:.2f} %\")\n",
    "\n",
    "print()\n",
    "print(f\"Overall tuned classification accuracy is: {np.mean(acc_fold_tune):.2f} %\")\n",
    "print()\n",
    "\n",
    "# Visualize summary of predictions\n",
    "multiclass_summary(np.concatenate(predictions_fold_tune, axis=0), np.concatenate(true_labels_fold_tune, axis=0), class_lab)\n",
    "print()\n",
    "\n",
    "# Save results\n",
    "results_tune = {\n",
    "    \"predictions\": predictions_fold,\n",
    "    \"true_labels\": true_labels_fold,\n",
    "    \"accuracy\": acc_fold,\n",
    "    \"best_params\": search.best_params_\n",
    "}\n",
    "\n",
    "file_name = results_dir_01 / 'RF_2PC_acc_tune.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(results_tune, f)\n",
    "\n",
    "print(f\"Data saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57956a0c-8cc7-49c8-a774-60a6b7980a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "As expected, tuning classifier parameters improves accuracy. Nevertheleess, using two PC's is arbitrary and is easy simply due to visualization purposes. We can also examine accuracy as a function of the number of PCs, and visualize the PC loading maps to interpret the classification results. But first, let's run some statistics to determine if the classification is significant in its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0231ff-6e49-44a1-80fe-ecdb19f148e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c9ebed-01df-4265-8b92-f8653547fc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We perform statistical testing to determine if the classification is significantly different (above) chance level. Here, we will do this using permutation testing where we randomly shuffle the training labels before building the classifier and then obtain accuracy values with the original testing labels. By randomly shuffling the training labels hundreds of times, we can build a null distribution and then observe where the \"true\" classification rate falls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dc29da-d673-4fb8-acd8-aa2af0a55009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since we will be doing a lot of iterations, let's first validate that parallel processing reducs computation time compared to serial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85186c0c-0ca5-4ce1-9d44-9069dea89457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for dask parallelization tasks\n",
    "data_pca_da = da.from_array(data_pca, chunks=(1000, data_pca.shape[1]))\n",
    "loadings_da = da.from_array(loadings, chunks=(loadings.shape[0], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82399334-dca9-47cb-9154-fc90aab623f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data set for testing parallelization (just one fold)\n",
    "data_pca_dask = np.matmul(data_pca_da, loadings_da[:,:2])\n",
    "load_train = data_load[train_idx_fold[0],:]\n",
    "train_labels = labels[train_idx_fold[0]]\n",
    "load_test = data_load[test_idx_fold[0],:]\n",
    "test_labels = labels[test_idx_fold[0]]\n",
    "\n",
    "# Serial for loop\n",
    "start_time = time.time()\n",
    "acc_shuffle_nodask = []\n",
    "for i in range(100):\n",
    "    acc_tmp = classification_RF_shuffle(load_train, train_labels, load_test, test_labels)\n",
    "    acc_shuffle_nodask.append(acc_tmp)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Parallel for loop (dask)\n",
    "start_time1 = time.time()\n",
    "acc_shuffle_dask = iteration_permute(load_train, train_labels, load_test, test_labels, n_iterations=100)   \n",
    "end_time1 = time.time()\n",
    "total_time1 = end_time1 - start_time1\n",
    "\n",
    "clear_output(wait=False)\n",
    "display(f\"Serial For Loop: {total_time:.2f} seconds\")\n",
    "display(f\"Parallel For Loop: {total_time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992562f1-cab6-4eae-b6e7-ae1c1e80b61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see that parallelization with dask markedly reduces the computation time (CPU) by ~34% when performing 100 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a274a8-0e7d-4004-9cef-b8eb4f42fa13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rather than performing all permutations on a single training/testing set, we split these iterations across the different folds. For example, when performing 500 permutations, we perform 100 using each of the five train/test set combinations. The null distribution is build by combining the results from these 5 x 100 permutations to ensure that local patterns in one train/test set do introduce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff9ea68-a750-4900-9c4d-e7585c81568b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Permute 100 times per fold for an even distribution\n",
    "start_time = time.time()\n",
    "\n",
    "data_pca_dask = np.matmul(data_pca_da, loadings_da[:,:2])\n",
    "acc_permute_fold = []\n",
    "for num_fold in range(len(train_idx_fold)):\n",
    "    \n",
    "    # Extract data per fold\n",
    "    load_train = data_load[train_idx_fold[num_fold],:]\n",
    "    train_labels = labels[train_idx_fold[num_fold]]\n",
    "    load_test = data_load[test_idx_fold[num_fold],:]\n",
    "    test_labels = labels[test_idx_fold[num_fold]]\n",
    "\n",
    "    acc_permute = []\n",
    "    for i in range(2):\n",
    "        acc_tmp=iteration_permute(load_train, train_labels, load_test, test_labels, n_iterations=50)\n",
    "        acc_permute = np.concatenate((acc_permute, acc_tmp), axis=0)\n",
    "\n",
    "    acc_permute_fold.append(acc_permute)\n",
    "\n",
    "acc_permute_tot = np.concatenate(acc_permute_fold, axis=0)\n",
    "\n",
    "plt.figure(figsize=(3, 4))\n",
    "b = plt.boxplot(acc_permute_tot)\n",
    "plt.title('Shuffled Classification Accuracy')\n",
    "plt.xlabel('Shuffled')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(30, 60)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time elapsed: {total_time:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Save results\n",
    "file_name = results_dir_01 / 'RF_2PC_acc_permute.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(acc_permute_tot, f)\n",
    "\n",
    "print(f\"Data saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed65ada2-3434-4240-9ebf-15e5459eb746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Determine significance of classification accuracy using a permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b286943-365b-4b97-9af6-ce36969a8127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "bucket_name=\"databricks-workspace-stack-brad-personal-bucket\"\n",
    "\n",
    "file_name = results_dir_01 / 'RF_2PC_acc.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "file_name = results_dir_01 / 'RF_2PC_acc_tune.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    results_tune = pickle.load(f)\n",
    "\n",
    "file_name = results_dir_01 / 'RF_2PC_acc_permute.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    acc_permute_tot = pickle.load(f)\n",
    "\n",
    "# Permutation p-value derivation\n",
    "accuracy = np.array(results['accuracy'])\n",
    "null_above_value = (acc_permute_tot > accuracy.mean()).sum()\n",
    "p_value = null_above_value/np.size(acc_permute_tot,0)\n",
    "display(f\"The p-value for the classification accuracy of {accuracy.mean():.2f}% is {p_value}\")\n",
    "\n",
    "accuracy_tune = np.array(results_tune['accuracy'])\n",
    "null_above_value_tune = (acc_permute_tot > accuracy_tune.mean()).sum()\n",
    "p_value_tune = null_above_value_tune/np.size(acc_permute_tot,0)\n",
    "display(f\"The p-value for the tuned classification accuracy of {accuracy_tune.mean():.2f}% is {p_value_tune}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8130cf-b2d7-4c08-95f7-20a9245ae340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Despite seeing a modest classifcation accuracy using two PC features, we see a significant result. However, this is obviously less than ideal and also highlights the pitfalls of letting statistical testing guide conclusions. Furthermore, using two PC's is arbitrary, and was performed here for computational and visual simplicity. We now explore classification as a function of the number of PC features used. Nevertheless, due to computational and time constraints, here default Random Forest parameters are used. Ideally, we would also tune parameters across folds like the example above; however, the goal here is really to identify trends in how the number of PCs affects the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e82591-0f51-4c47-863f-9827730efe70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "num_pc=np.linspace(1,50,50, dtype = 'int')\n",
    "\n",
    "acc_pc_tot = []\n",
    "for num_fold in range(len(train_idx_fold)):\n",
    "\n",
    "    # Extract data per fold\n",
    "    train_data = data_pca[train_idx_fold[num_fold],:]\n",
    "    train_labels = labels[train_idx_fold[num_fold]]\n",
    "    test_data = data_pca[test_idx_fold[num_fold],:]\n",
    "    test_labels = labels[test_idx_fold[num_fold]]\n",
    "\n",
    "    acc_pc=iteration_PCs(train_data, train_labels, test_data, test_labels, loadings_da, num_pc)\n",
    "    acc_pc_tot.append(acc_pc)\n",
    "\n",
    "acc_pc_tot = np.vstack(acc_pc_tot)\n",
    "\n",
    "clear_output(wait=False)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time elapsed: {total_time:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Save results\n",
    "file_name = results_dir_01 / 'RF_NUM_PCS_acc.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(acc_pc_tot, f)\n",
    "\n",
    "print(f\"Data saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e32f7d-ba83-4d91-a4c5-69e21883b1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "file_name = results_dir_01 / 'RF_NUM_PCS_acc.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    acc_pc_tot = pickle.load(f)\n",
    "\n",
    "# Plot accuracy as a function of number of PCs\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "for num_fold in range(len(acc_pc_tot)):\n",
    "    plt.plot(num_pc, acc_pc_tot[num_fold], 'gray', linewidth=0.75, label=f'Folds' if num_fold == 0 else \"\")\n",
    "\n",
    "plt.plot(num_pc, acc_pc_tot.mean(axis=0), 'k', linewidth=2, label='Average Accuracy')\n",
    "\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Number of PCs')\n",
    "plt.ylim(30, 100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad4e16b-cb8d-48f8-9478-6d5249e8591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As expected, here we see a progressive improvement in classification with more and more PC features (peaking at >90%) that is also consistent across folds. We also need to create a null distribution in the same way as before as a function of PC features used to determine statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f274d29-3cec-4973-9e72-14e5cd9fca02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Permute 100 times per fold (5x), per PC \n",
    "start_time = time.time()\n",
    "\n",
    "num_pc = np.linspace(2, 50, 25, dtype='int')\n",
    "\n",
    "acc_pc_permute_tot = []\n",
    "for num_pc in num_pc:  # PC\n",
    "\n",
    "    acc_pc_permute_pc = np.array([])\n",
    "    for num_fold in range(len(train_idx_fold)): # Fold\n",
    "        \n",
    "        train_data = data_pca[train_idx_fold[num_fold],:]\n",
    "        train_labels = labels[train_idx_fold[num_fold]]\n",
    "        test_data = data_pca[test_idx_fold[num_fold],:]\n",
    "        test_labels = labels[test_idx_fold[num_fold]]\n",
    "\n",
    "        acc_pc_permute_fold = np.array([])\n",
    "        for i in range(2):  # Iteration within pc\n",
    "            acc_tmp = iteration_permute_PCs(train_data, train_labels, test_data, test_labels, loadings_da, num_pc, n_iterations=50)\n",
    "            acc_pc_permute_fold=np.concatenate((acc_pc_permute_fold, acc_tmp), axis=0)\n",
    "\n",
    "        acc_pc_permute_pc=np.concatenate((acc_pc_permute_pc, acc_pc_permute_fold), axis=0)\n",
    "\n",
    "    acc_pc_permute_tot.append(acc_pc_permute_pc)\n",
    "\n",
    "acc_pc_permute_tot = np.vstack(acc_pc_permute_tot)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Save results\n",
    "file_name = results_dir_01 / 'RF_NUM_PCS_acc_permute.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(acc_pc_permute_tot, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a40b276-ba0f-4a2f-beb0-651b9a12073d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot accuracy and null distributions (with statistical results) as a function of the number of PC features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0a25cb-a44e-4912-90c1-134a03e80535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "file_name = results_dir_01 / 'RF_NUM_PCS_acc_permute.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    acc_pc_permute_tot = pickle.load(f)\n",
    "\n",
    "file_name = results_dir_01 / 'RF_NUM_PCS_acc.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    acc_pc_tot = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# Plot accuracy as a function of number of PCs\n",
    "num_pc = np.linspace(2, 50, 25, dtype='int')\n",
    "acc_pc_tot = np.mean(acc_pc_tot, axis=0)\n",
    "acc_pc_tot = acc_pc_tot[num_pc-1]\n",
    "ax.plot(num_pc, acc_pc_tot, linestyle='-', color='k', linewidth=2)\n",
    "\n",
    "# perform permutation testing and plot null distribution\n",
    "p_val_tot = []\n",
    "for i in range(np.size(acc_pc_permute_tot, 0)):\n",
    "    b = plt.boxplot(acc_pc_permute_tot[i], 'b', positions=[num_pc[i]], flierprops={'marker': '.', 'markersize': 5})\n",
    "\n",
    "    # Permutation testing\n",
    "    null_above_value = (acc_pc_permute_tot[i] > acc_pc_tot[i]).sum()\n",
    "    p_val_tot.append(null_above_value/np.size(acc_pc_permute_tot[i],0))\n",
    "\n",
    "# Visualize p-values\n",
    "p_val_tot = np.array([float(num) for num in p_val_tot])\n",
    "p_value_adj = false_discovery_control(p_val_tot)   \n",
    "\n",
    "for i in range(len(p_value_adj)): \n",
    "    if p_value_adj[i] < 0.05:\n",
    "        plt.scatter(num_pc[i], 95, s = 10, c = 'r', marker = \"*\")\n",
    "\n",
    "ax.set_xlim(0, 55)\n",
    "ax.set_xticks(range(0, 56, 5))\n",
    "ax.set_xticklabels(range(0, 56, 5))\n",
    "plt.ylim(30, 100)\n",
    "ax.set_title('Number of Principal Components vs Accuracy')\n",
    "ax.set_xlabel('Number of Principal Components')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b1a48c-0b1a-4ca9-add8-38b42a158d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is reassuring to see that the null distribution, or chance level, remains quite constant even when using more PC features. This provides even more confidence that the increasing classification accuracy is due to class-specific differences rather than chance. Given that the previous accuracy of ~48% was significant, it is also expected that all other results are also significant (red asterisks) even when accounting for multiple comparisons (i.e. false-discovery rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd55ec8c-70a6-466c-9843-f56fc99e2806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, 7, figsize=(9, 6))\n",
    "lim = 21\n",
    "for ipc in range(lim):\n",
    "\n",
    "    PC_map = loadings[:,ipc]\n",
    "    PC_map = np.reshape(PC_map, (128,128))\n",
    "    clim = max(abs(np.min(PC_map)), abs(np.max(PC_map)))\n",
    "\n",
    "    idx1 = np.floor(ipc/7).astype(int)\n",
    "    idx2 = np.fmod(ipc,7).astype(int)\n",
    "    ax[idx1, idx2].axis('off')\n",
    "    ax[idx1, idx2].imshow(PC_map, cmap = \"magma\", vmin = -clim, vmax = clim)\n",
    "    ax[idx1, idx2].set_title(f'PC# {ipc +1}', fontsize = 12)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f923ef6c-0aab-44d9-9bf9-58fc7705d88f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we can see that for each of the top 21 PC's some brain regions are more strongly weighted than others (in either the positive or negative direction). It is important to note that it does not appear that each image used in this dataset came from the same location/slice within the brain. We do not know if certain slices are over- or under-represented in different AD classes and therefore cannot say with certainty whether specific anatomical structures drive different stages of AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f8f2a9-b719-4584-92d2-36ca780153ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate average maps for first X PCs\n",
    "loadings_abs = np.abs(loadings)\n",
    "averages = [np.mean(loadings_abs[:, :n], axis=1) for n in [2, 25, 50]]\n",
    "images = [avg.reshape(128, 128) for avg in averages]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "titles = ['Average of First 2 Loadings', 'Average of First 25 Loadings', 'Average of First 50 Loadings']\n",
    "\n",
    "for ax, img, title in zip(axes, images, titles):\n",
    "    ax.imshow(img, cmap='magma')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde086ee-3fce-4ccf-8e11-a3cc84cac9ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When examining cumulative absolute PC maps, it is clear that discriminatory information focuses on the lateral ventricles. However, improvements in classification are also linked with more lateral brain regions that become more apparent in the 25 and 50 PC maps that help achieve ~90% accuracy."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_random_forest",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
